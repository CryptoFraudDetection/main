---
title: Setup
jupyter: python3
---

```{python}
import contextlib
import heapq
import json
import re
from pathlib import Path

import altair as alt
import matplotlib.pyplot as plt
import networkx as nx
import pandas as pd
from community import community_louvain
from IPython.display import Image, display
from tqdm.notebook import tqdm

alt.themes.enable("dark")
```

Read data from parquet file:

```{python}
df = pd.read_parquet("../data/processed/reddit.parquet")
with Path("../data/raw/coins.json").open() as f:
    coins = json.load(f)

# drop stoa network from coins as it has no comments
coins = [coin for coin in coins if coin["name"] != "STOA Network"]

train_coins = [coin["name"] for coin in coins if not coin["test"]]
fraud_coins = [coin["name"] for coin in coins if coin["fraud"]]
nonfraud_coins = [coin["name"] for coin in coins if not coin["fraud"]]
df = df[df["search_query"].isin(train_coins)].copy()

datetime_columns = ["edited", "created"]
df[datetime_columns] = df[datetime_columns].apply(
    pd.to_datetime,
    errors="coerce",
)
```

## EDA

### Data Overview

Glimpse of the data:

```{python}
df.sample(5, random_state=42)
```

Some rows are comments and others are posts: Posts `depth` is set to -1 and they are missing `parent_id` as they initiate a potential discussion. Comments have `parent_id` set to the `id` of the post they are replying to and they are missing `url`, `num_comments`, `title` as they are not posts. Comments can be replies to posts or other comments and therefore a nested structure can be formed.

```{python}
df.dtypes
```

These are the columns in the data. Our Network will be an OneMode Network with `author` nodes and the directed edges being the comments (`depth>=0`).

#### Crypto Coins Frequency

How many posts are there for each coin?

```{python}
df.query("depth == -1")["search_query"].value_counts()
```

Some coins like Teddy Doge and BeerCoin have only a few posts. How many comments are there for each coin?

```{python}
df.query("depth > -1")["search_query"].value_counts()
```

There are multiple hundreds of commeents for each coin or more. As the comments are more important to this analysis and the coins are therefore kept.

#### Score Distribution

How are the scores distributed over the posts and comments in respect to there depth (reply to reply to reply etc.)? Note that posts have `depth` set to `-1`.

```{python}
depths = range(-1, 10)  # Tiefen von -1 bis 9
scores_by_depth = [
    df[df["depth"] == depth]["score"].dropna() for depth in depths
]

means_by_depth = [scores.mean() for scores in scores_by_depth]

# Erstelle die Boxplots
plt.figure(figsize=(12, 8))
plt.boxplot(
    scores_by_depth,
    vert=False,
    patch_artist=True,
    tick_labels=[f"Depth {depth}" for depth in depths],
    boxprops=dict(facecolor="lightblue", color="blue"),
    medianprops=dict(color="red", linewidth=1.5),
    flierprops=dict(marker="o", color="orange", markersize=5, alpha=0.5),
)

# Zeichne vertikale Linien f√ºr die Mittelwerte ein
for i, mean in enumerate(means_by_depth):
    plt.axvline(
        x=mean,
        ymin=(i + 0.25) / len(depths),
        ymax=(i + 0.75) / len(depths),
        color="green",
        linewidth=1.5,
        label="Mean" if i == 0 else "",
    )

# Plot-Details
plt.title("Boxplot of Scores by Depth (with Mean)", fontsize=14)
plt.xlabel("Score (Log scaled)", fontsize=12)
plt.ylabel("Depth", fontsize=12)
plt.xscale("log")
plt.grid(axis="x", linestyle="--", alpha=0.7)
plt.legend()
plt.show()
```

In this plot its clearly visible, that deeper nested comments usually have a lower score. Posts have extreme outliers, which is visible in the logaritmic X-axis. The Distribution of the score is right-skewed because the mean is higher than the median.

#### Comments and Posts Distributions per Coin and Subreddit

Overview of comments (answers to other people) per coin and subreddit:

```{python}
subreddit_query = (
    df.groupby(["search_query", "subreddit"])
    .size()
    .unstack()
    .fillna(0)
    .astype(int)
    .reset_index()
    .melt(id_vars="search_query")
)

c = (
    alt.Chart(subreddit_query)
    .mark_bar()
    .encode(
        x=alt.X(
            "value:Q",
            title="Number of Comments",
            scale={"domain": [0, 152_000]},
        ),
        y=alt.Y("search_query:O", title="Coin"),
        color=alt.Color("subreddit:N", title="Subreddit"),
        tooltip=[
            alt.Tooltip("search_query:O", title="Coin"),
            alt.Tooltip("subreddit:N", title="Subreddit"),
            alt.Tooltip("value:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Comments per Subreddit"),
    )
)


text = (
    alt.Chart(subreddit_query)
    .mark_text(align="left", dx=5, color="white")
    .transform_calculate(customtooltip="datum.value")
    .encode(
        x=alt.X("sum(value):Q", scale={"domain": [0, 152_000]}),
        y=alt.Y("search_query:O"),
        text=alt.Text("sum(value):Q"),
    )
)

c + text
```

It's clearly visible that Bitcoin is the most popular coin and CryptoCurrency is the most popular subreddit. The interest is, how fraudulent and non-fraudulent coins differ in there social media activity. Some coins (like Teddy Doge and BeerCoin) have only a few comments.

Overview of main posts per coin and subreddit:

```{python}
subreddit_query_posts = (
    df.query("depth == -1")
    .groupby(["search_query", "subreddit"])
    .size()
    .unstack()
    .fillna(0)
    .astype(int)
    .reset_index()
    .melt(id_vars="search_query")
)

c = (
    alt.Chart(subreddit_query_posts)
    .mark_bar()
    .encode(
        x=alt.X(
            "value:Q",
            title="Number of Comments",
            scale={"domain": [0, 1_400]},
        ),
        y=alt.Y("search_query:O", title="Coin"),
        color=alt.Color("subreddit:N", title="Subreddit"),
        tooltip=[
            alt.Tooltip("search_query:O", title="Coin"),
            alt.Tooltip("subreddit:N", title="Subreddit"),
            alt.Tooltip("value:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Posts per Subreddit"),
    )
)


text = (
    alt.Chart(subreddit_query_posts)
    .mark_text(align="left", dx=5, color="white")
    .transform_calculate(customtooltip="datum.value")
    .encode(
        x=alt.X("sum(value):Q", scale={"domain": [0, 1_400]}),
        y=alt.Y("search_query:O"),
        text=alt.Text("sum(value):Q"),
    )
)

c + text
```

The distribution of posts is similar to the distribution of comments. Teddy Doge got only a few posts. This should not be an issue, as the comments (replies to other people) will be the edges in the graph.

#### Comments Depth Distribution

How deeply nested are the comments (replies to other people)?

```{python}
depth_df = df["depth"].value_counts().sort_index()

c = (
    alt.Chart(depth_df.reset_index())
    .mark_bar()
    .encode(
        x=alt.X(
            "depth:O",
            title="Depth (-1 is the original post and 0 are top-level comments)",
        ),
        y=alt.Y(
            "count:Q",
            title="Number of Comments",
            scale={"domain": [0, 240_000]},
        ),
        tooltip=[
            alt.Tooltip("depth:O", title="Depth"),
            alt.Tooltip("count:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Comments per Depth"),
    )
)

text = (
    alt.Chart(depth_df.reset_index())
    .mark_text(align="center", dy=-8, color="white")
    .transform_calculate(customtooltip="datum.count")
    .encode(
        x=alt.X("depth:O"),
        y=alt.Y("count:Q", scale={"domain": [0, 240_000]}),
        text=alt.Text("count:Q"),
    )
)

c + text
```

There are ~6k posts and most comments (~200k) are direct replies to the post. The distribution decreases ~exponentially and goes to 10 levels deep.

#### Comment/Post Count Distributions per User

How are the number of comments distributed per user?

```{python}
comments_per_user = df.groupby("author").size()
distribution = comments_per_user.value_counts().sort_index()
distribution_df = distribution.reset_index()
distribution_df.columns = ["comments_per_user", "number_of_users"]

c = (
    alt.Chart(distribution_df)
    .mark_bar()
    .encode(
        x=alt.X(
            "comments_per_user:O",
            title="Number of Comments per User (cut off at 100)",
            scale={"domain": range(1, 101)},
        ),
        y=alt.Y(
            "number_of_users:Q",
            title="Number of Users",
            scale=alt.Scale(
                type="symlog",
                domain=[0, 100_000],
            ),
            axis=alt.Axis(
                values=[
                    0,
                    1,
                    2,
                    5,
                    10,
                    20,
                    50,
                    100,
                    200,
                    500,
                    1_000,
                    2_000,
                    5_000,
                    10_000,
                    20_000,
                    50_000,
                    100_000,
                ],
            ),
        ),
        tooltip=[
            alt.Tooltip("comments_per_user:Q", title="# of Comments per User"),
            alt.Tooltip("number_of_users:Q", title="# of Users"),
        ],
    )
    .properties(
        width=1000,
        height=400,
        title=alt.Title(text="Number of Comments per User"),
    )
)

c
```

Most users have only a few comments (~70k have only one comment). There are some users with a lot of comments (cut off at 100). How extreme are the top users?

```{python}
comments_per_user.sort_values(ascending=False).head(10)
```

Some users have a lot of comments but they have `bot`, `auto` or `mod` in their name, they are therefore probably no human. They might be official bots from the subreddit for moderation purposes.

How the posts are distributed over the users:

```{python}
posts_per_user = df.query("depth == -1").groupby("author").size()
distribution = posts_per_user.value_counts().sort_index()
distribution_df = distribution.reset_index()
distribution_df.columns = ["posts_per_user", "number_of_users"]

c = (
    alt.Chart(distribution_df)
    .mark_bar()
    .encode(
        x=alt.X(
            "posts_per_user:O",
            title="Number of Posts per User (cut off at 100)",
            scale={"domain": range(1, 101)},
        ),
        y=alt.Y(
            "number_of_users:Q",
            title="Number of Users",
            scale=alt.Scale(
                type="symlog",
                domain=[0, 5_000],
                zero=True,
            ),
            axis=alt.Axis(
                values=[
                    0,
                    1,
                    2,
                    5,
                    10,
                    20,
                    50,
                    100,
                    200,
                    500,
                    1_000,
                    2_000,
                    5_000,
                ],
            ),
        ),
        tooltip=[
            alt.Tooltip("posts_per_user:Q", title="# of Posts per User"),
            alt.Tooltip("number_of_users:Q", title="# of Users"),
        ],
    )
    .properties(
        width=1000,
        height=400,
        title=alt.Title(text="Number of Posts per User"),
    )
)

c
```

The posts are distributed unevenly as well. ~3.6k users have only one post. How extreme are the top users?

```{python}
posts_per_user.sort_values(ascending=False).head(10)
```

Some users overlap with the top commenters. Examples: `kirtash93`, `goldyluckinblokchain`

#### Comment/Post Distributions over Time

How are the comments and posts distributed over time?

```{python}
posts_per_coin_per_date = df.copy()
posts_per_coin_per_date["created"] = (
    pd.to_datetime(posts_per_coin_per_date["created"])
    .dt.to_period("M")
    .dt.to_timestamp()
)

posts_per_coin_per_date = posts_per_coin_per_date.query(
    "created >= '2020-01-01'",
).query("depth >= 0")

posts_per_coin_per_date = (
    posts_per_coin_per_date.groupby(["search_query", "created"])["id"]
    .count()
    .reset_index()
    .rename(columns={"id": "number_of_comments"})
)

posts_per_coin_per_date["number_of_comments_rel"] = posts_per_coin_per_date[
    "number_of_comments"
] / posts_per_coin_per_date.groupby("search_query")[
    "number_of_comments"
].transform(
    "sum",
)

step = 40
overlap = 1

c = (
    alt.Chart(posts_per_coin_per_date, height=step)
    .mark_area(
        interpolate="monotone",
        fillOpacity=0.5,
        stroke="lightgray",
        strokeWidth=0.5,
    )
    .encode(
        x=alt.X("created:T").title("Date").axis(grid=False),
        y=alt.Y("number_of_comments_rel:Q")
        .axis(None)
        .scale(range=[step, -step * overlap]),
        color=alt.Color("search_query:N", legend=None),
        tooltip=[
            alt.Tooltip("yearmonth(created):T", title="Date"),
            alt.Tooltip("search_query:N", title="Coin"),
            alt.Tooltip("number_of_comments:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=step,
    )
    .facet(
        row=alt.Row("search_query:N")
        .title(None)
        .header(labelAngle=0, labelAlign="left"),
    )
    .properties(
        title=alt.Title(
            text="Number of Comments per Coin per Date after 2020",
            anchor="middle",
        ),
        bounds="flush",
    )
    .configure_facet(spacing=0)
    .configure_view(stroke=None)
    .configure_title(anchor="end")
)

c
```

```{python}
posts_per_coin_per_date = df.copy()
posts_per_coin_per_date["created"] = (
    pd.to_datetime(posts_per_coin_per_date["created"])
    .dt.to_period("M")
    .dt.to_timestamp()
)

posts_per_coin_per_date = posts_per_coin_per_date.query(
    "created >= '2020-01-01'",
).query("depth == -1")


posts_per_coin_per_date = (
    posts_per_coin_per_date.groupby(["search_query", "created"])["id"]
    .count()
    .reset_index()
    .rename(columns={"id": "number_of_posts"})
)

posts_per_coin_per_date["number_of_posts_rel"] = posts_per_coin_per_date[
    "number_of_posts"
] / posts_per_coin_per_date.groupby("search_query")[
    "number_of_posts"
].transform("sum")

posts_per_coin_per_date

step = 40
overlap = 1

c = (
    alt.Chart(posts_per_coin_per_date, height=step)
    .mark_area(
        interpolate="monotone",
        fillOpacity=0.5,
        stroke="lightgray",
        strokeWidth=0.5,
    )
    .encode(
        x=alt.X("created:T").title("Date").axis(grid=False),
        y=alt.Y("number_of_posts_rel:Q")
        .axis(None)
        .scale(range=[step, -step * overlap]),
        color=alt.Color("search_query:N", legend=None),
        tooltip=[
            alt.Tooltip("yearmonth(created):T", title="Date"),
            alt.Tooltip("search_query:N", title="Coin"),
            alt.Tooltip("number_of_posts:Q", title="# of Posts"),
        ],
    )
    .properties(
        width=800,
        height=step,
    )
    .facet(
        row=alt.Row("search_query:N")
        .title(None)
        .header(labelAngle=0, labelAlign="left"),
    )
    .properties(
        title=alt.Title(
            text="Number of Posts per Coin per Date after 2020",
            anchor="middle",
        ),
        bounds="flush",
    )
    .configure_facet(spacing=0)
    .configure_view(stroke=None)
    .configure_title(anchor="end")
)

c
```

The plots are cut off at 2020 because there are not many posts or comments before that time (in relation to the rest of the data). Some coins have more data at the end of the timescale because we used google results to get the post URLs and google had probably some bais towards newer posts.

### NA Values

#### Posts

How many posts are there?

```{python}
print("Number of posts:", len(df.query("depth == -1")))
```

`parent_id` is expeted to be NA for all posts. Columns with missing values for posts:

```{python}
df.query("depth == -1").isna().sum()[lambda x: x > 0]
```

Some posts have been edited but this is ignored in this analysis.

#### Comments

How many comments are there?

```{python}
print("Number of comments:", len(df[df["depth"] >= 0]))
```

`title`, `url` and `num_comments` are not used for comments and are expected to be NA. Which columns do have missing values for comments?

```{python}
df.query("depth >= 0").isna().sum()[lambda x: x > 0]
```

`edited` is ignored for comments as well.

Posts are not of primary interest for this analysis and therefore unnecessary columns are dropped (even though they are not always NA for posts):

```{python}
columns = ["edited", "title", "url", "num_comments"]
shrinked_df = df.drop(columns=columns)
print("Dropped columns:")
print(columns)
print("Columns remaining:")
print(shrinked_df.columns.to_list())
```

## User Interaction Network

In this analysis the interactions between users is analyzed and therefore a column `paprent_user` is created with the `author` of the post/comment the user is replying to. This enables the creation of a graph with users as nodes and directed edges being the comments.

### Data Preprocessing

The data is grouped by `author` and `parent_user` so there is only one edge between the same nodes. The edge weights are the number of comments between the two authors. Edges get `sum(score)`, sum of critical word hit counts (example: `moon`) and number of coin occurrences as additional edge attributes.

As some algorithms do not work on directed graphs it might be necessary to convert the graph to an undirected graph later on. The information in which direction the comments went would be lost, but the overall information, which users tend to interact and the word counts will be preserved.

The aggregation is done on the data frame and a glimpse at the data is provided (sorted by some edge attribute):

```{python}
# Add parent user column (author of the comment/post the user is replying to)
parent_user_df = shrinked_df.copy().merge(
    shrinked_df[["id", "author"]].rename(
        columns={"id": "parent_id", "author": "parent_user"},
    ),
    on="parent_id",
    how="left",
)

# Keyword word hit count
patterns = {"moon": r"moon", "pump": r"pump"}
keyword_df = parent_user_df.copy()
for col, pattern in patterns.items():
    keyword_df[col] = (
        keyword_df["body"].str.findall(pattern, flags=re.IGNORECASE).str.len()
    )

# Numeric columns will be aggregated later on for comments and posts
agg_columns = keyword_df.select_dtypes("number").columns.to_list()
agg_columns.remove("depth")  # not of interest to aggregated edges

# Filter for comments
comments_df = keyword_df.query("depth >= 0").copy()


def group_comments_df(comments_df: pd.DataFrame, coin: str) -> pd.DataFrame:
    comments_df = comments_df.query(f"search_query == '{coin}'")
    # Group and aggregate comments
    return comments_df.groupby(
        ["author", "parent_user"],
        as_index=False,
    ).agg(
        weight=("score", "size"),
        mean_score=("score", "mean"),
        **{col: (col, "sum") for col in agg_columns},
    )


grouped_comment_dfs = [
    group_comments_df(comments_df, coin) for coin in train_coins
]
```

TODO: update text

Edges between users with unknown user names are high up in the `count` and `weight` scores as they are all grouped together. Their comments are kept for now, as they might give insight, but it is important to note, that probably multiple users where put together into `None`. Edges between users with unknown names will be kept as a self loop edge on the node `None`.

`None` users and users with `mod`, `auto` or `bot` in there names are high in the list of edges sorted by `moon` or `pump` count. This might suggest, that they might not be official mods, because official mods would probably not use those words (this would have to be verified).

The edge data is now aggregated to precalculate node attributes to speed up network analysis. For each node in and out sums of edge attributes are calculated by grouping by `author` or `parent_user` accordingly. The attributes are prefixed with `out_comment` and `in_comment` accordingly. The number of posts (`post_count`), sum of scores of posts (`post_scores`) and selected word counts in posts are aggregated per author as well. A glimpse at the node data is provided:

```{python}
def prefix_column(
    df: pd.DataFrame,
    prefix: str,
    exclude_column: str = "author",
) -> pd.DataFrame:
    """Prefixes all column names in the DataFrame except one with a given string.

    Args:
        df: The DataFrame whose columns will be renamed.
        prefix: The string to add as a prefix to the column names.
        exclude_column: The name of the column to exclude from renaming.

    Returns:
        A new DataFrame with renamed columns.

    """
    return df.rename(
        columns={
            col: f"{prefix}{col}"
            for col in df.columns
            if col != exclude_column
        },
    )


grouped_out_comments_dfs = []
grouped_in_comments_dfs = []

for coin_df in grouped_comment_dfs:
    # Outgoing edge stats per node
    grouped_out_comments_dfs.append(
        prefix_column(
            (
                coin_df.drop(columns="parent_user")
                .groupby("author", as_index=False)
                .sum()
            ),
            "out_comment_",
        ),
    )

    # Incomming edge stats per node
    grouped_in_comments_dfs.append(
        prefix_column(
            (
                coin_df.drop(columns="author")
                .groupby("parent_user", as_index=False)
                .sum()
                .rename(columns={"parent_user": "author"})
            ),
            "in_comment_",
        ),
    )


def group_post_df(post_df: pd.DataFrame, coin: str) -> pd.DataFrame:
    return prefix_column(
        (
            post_df.query(f"search_query == '{coin}'")
            .groupby("author", as_index=False)
            .agg(
                count=("score", "size"),
                mean_score=("score", "mean"),
                **{col: (col, "sum") for col in agg_columns},
            )
        ),
        "post_",
    )


post_df = keyword_df.query("depth == -1")

grouped_post_dfs = [group_post_df(post_df, coin) for coin in train_coins]

node_dfs = []
for grouped_comments_df, grouped_in_comments_df, grouped_posts_df in zip(
    grouped_out_comments_dfs,
    grouped_in_comments_dfs,
    grouped_post_dfs,
    strict=False,
):
    node_dfs.append(
        grouped_posts_df.merge(grouped_comments_df, "outer")
        .merge(grouped_in_comments_df, "outer")
        .fillna(0),
    )
```

```{python}
btc_graph_df = node_dfs[0]
```

### Network Graph Creation

The directed graph is created:

```{python}
def create_graph_from_dataframe(
    df: pd.DataFrame,
    name_column: str = "author",
) -> nx.Graph:
    """Create a NetworkX graph with nodes from a pandas DataFrame.

    One column is used as the node name and all other columns become node
    attributes.

    Args:
        df: The pandas DataFrame containing the data.
        name_column: The column to use as the node name.

    Returns:
        A NetworkX graph with nodes and their attributes.

    """
    # Initialize a graph
    G = nx.Graph()

    # Set the graph name to the column containing the network name
    G.graph["name"] = "Graph based on column: " + name_column

    # Add nodes with attributes
    G.add_nodes_from(
        (row[name_column], row.drop(name_column).to_dict())
        for _, row in df.iterrows()
    )

    return G


graphs = {}
for i, coin in enumerate(train_coins):
    graph_file_path = f"../data/processed/{coin}.gexf"

    G = None
    with contextlib.suppress(Exception):
        G = nx.read_gexf(graph_file_path)
        print(f"Graph loaded from {graph_file_path}")

    if G is None:
        G = create_graph_from_dataframe(node_dfs[i])

        for _, row in grouped_comment_dfs[i].iterrows():
            edge_attributes = row[2:].to_dict()
            G.add_edge(row["author"], row["parent_user"], **edge_attributes)

        nx.write_gexf(G, graph_file_path)
        print(f"Graph saved as: {graph_file_path}")
    graphs[coin] = G
```

The top nodes for each edge attribute are visualized to get some insights:

```{python}
def visualize_top_nodes_graphviz(
    graph: nx.DiGraph,
    attribute: str,
    top_n: int = 10,
    output_file: str = "graph",
) -> None:
    """Visualize the top N nodes based on a specified node attributes.

    Nodes are visualized with their directed edges for visualization with
    Graphviz. Ensures arrows are displayed to indicate edge direction.

    Args:
        graph: The NetworkX directed graph.
        attribute: The node attribute to use for ranking.
        top_n: The number of top nodes to include.
        output_file: The base name for saving DOT and PNG files.

    """
    # Ensure the attribute exists in the nodes
    if not all(attribute in graph.nodes[node] for node in graph.nodes):
        msg = f"Attribute '{attribute}' not found in all nodes."
        raise ValueError(msg)

    # Find the top N nodes by the specified attribute
    top_nodes = [
        node
        for node, _ in heapq.nlargest(
            top_n,
            graph.nodes(data=attribute),
            key=lambda x: x[1],
        )
    ]

    # Create a subgraph with only the top nodes (includes isolated nodes)
    subgraph = graph.subgraph(top_nodes)

    # Convert to PyGraphviz AGraph and force directed rendering
    A = nx.nx_agraph.to_agraph(subgraph)
    A.graph_attr["rankdir"] = "TB"  # Layout top-to-bottom
    A.graph_attr["label"] = f"Top {top_n} Nodes by {attribute.capitalize()}"
    A.graph_attr["labelloc"] = "top"
    A.graph_attr["fontsize"] = 20

    # Ensure directed graph and arrowheads
    A.graph_attr["directed"] = "true"
    for edge in A.edges():
        edge.attr["arrowhead"] = "normal"  # Explicitly set arrowhead for edges

    # Export DOT file and render as PNG
    dot_file = f"{output_file}.dot"
    png_file = f"{output_file}.png"
    A.write(dot_file)
    A.layout(prog="dot")  # Use Graphviz's dot layout
    A.draw(png_file)

    # Display the PNG in the notebook
    display(Image(filename=png_file))


def get_node_attributes_from_df(
    df: pd.DataFrame,
    keywords: tuple[str, ...] = ("count", "weight", "mean", "pump", "dump"),
) -> list[str]:
    """Extract column names from df that contain any of the keywords.

    Args:
        df: The DataFrame from which to extract column names.
        keywords: A tuple of keywords to look for in column names.

    Returns:
        list: A list of column names from the DataFrame that contain one or more
            of the specified keywords.

    """
    return [
        attr
        for attr in df.columns.to_list()
        if any(keyword in attr for keyword in keywords)
    ]


# for node_attr in get_node_attributes_from_df(node_df):
#     visualize_top_nodes_graphviz(G, node_attr)
```

Notes:
- filter z.b. kanten mit min 10x bitcoin
- fraud vs non-fraud
- innerhalb von fraud oder non-fraud vergleichen
- viel mit filter arbeiten um kleinere netzwerke zu haben
- gephi f√ºr explorativ
    - gexf exportieren und dann in notebook laden
    - kann auch plot exportieren und dann in notebook laden
    - bei filtern √∂ffnen als neuen workspace
    - speichern als gexf nicht als gephi
- gelerntes anwenden
    - ziegen dass man es auch interpretieren kan
- attribute n_fraud/nonfraud_comments/post
- was wir in der theorie gelernt haben zeigen wir nun dass wir es anwenden k√∂nnen und dass wir daraus lesen k√∂nnen und dass wir begr√ºndet
- zentralit√§tsmass und communities und das f√ºr non-fraud vs fraud, und noch mehr
- visualisierung ist wichtig
- statistik, hypothesentests
- um so zentraler desto eher fraud z.B. 
- ausblick mit was man noch machen kann


## Analysis

Are there Users, which answer often to the same users?

These are the User with most comments to the same user. There are many Moderator bots in the list and also None.

```{python}
def calculate_communities(graph) -> dict:
    """Calculate the communities in a graph using the Louvain algorithm.

    If the graph is directed, it is first converted to an undirected graph.

    Parameters
    ----------
        graph (networkx.Graph): The input graph.

    Returns
    -------
        dict: A dictionary mapping each node to its community ID.

    """
    # Convert to undirected graph if necessary
    if isinstance(graph, nx.DiGraph):  # Check if the graph is directed
        graph = graph.to_undirected()

    # Apply the Louvain algorithm
    communities = community_louvain.best_partition(graph)

    # Calculate modularity
    modularity = community_louvain.modularity(communities, graph)

    return communities, modularity


def calculate_density(graph) -> float:
    """Calculate the density of a graph.

    If the graph is directed, it is first converted to an undirected graph.

    Parameters
    ----------
        graph (networkx.Graph): The input graph.

    Returns
    -------
        float: The density of the graph, a value between 0 and 1.

    """
    # Convert to undirected graph if necessary
    if isinstance(graph, nx.DiGraph):  # Check if the graph is directed
        graph = graph.to_undirected()

    # Calculate the density
    density = nx.density(graph)
    return density


def calculate_degree_centrality(graph) -> float:
    """Calculate the degree centrality for each node in the graph.

    Parameters
    ----------
        graph (networkx.Graph): The input graph.

    Returns
    -------
        dict: A dictionary mapping each node to its degree centrality value.

    """
    # Convert to undirected graph if necessary
    if isinstance(graph, nx.DiGraph):  # Check if the graph is directed
        graph = graph.to_undirected()

    # Calculate degree centrality
    degree_centrality = nx.degree_centrality(graph)
    return degree_centrality


communities = {}
modularties = {}
for coin, graph in tqdm(graphs.items()):
    communities_, modularities_ = calculate_communities(graph)
    communities[coin] = communities_
    modularties[coin] = modularities_
densities = {
    coin: calculate_density(graph) for coin, graph in tqdm(graphs.items())
}

degree_centralities = {
    coin: calculate_degree_centrality(graph)
    for coin, graph in tqdm(graphs.items())
}

# Calculate the number of nodes in each community
community_sizes = {}
for coin, communities_ in communities.items():
    community_sizes[coin] = {}
    for user in communities_:
        community_id = communities_[user]
        if community_id not in community_sizes[coin]:
            community_sizes[coin][community_id] = 0
        community_sizes[coin][community_id] += 1

# Convert the data into a DataFrame for plotting
rows = []
for coin, communities_ in community_sizes.items():
    for community_id, size in communities_.items():
        rows.append({"coin": coin, "community_id": community_id, "size": size})
community_sizes_df = pd.DataFrame(rows)

# Prepare data for the boxplot
boxplot_data = [
    community_sizes_df[community_sizes_df["coin"] == coin]["size"]
    for coin in community_sizes_df["coin"].unique()
]

# Colors for coin names
coin_colors = [
    "red" if coin in fraud_coins else "darkblue"
    for coin in community_sizes_df["coin"].unique()
]

# Create the boxplot
plt.figure(figsize=(10, 6))
box = plt.boxplot(
    boxplot_data,
    tick_labels=community_sizes_df["coin"].unique(),
    patch_artist=True,
)

# Color the x-axis labels
ax = plt.gca()
for label, color in zip(ax.get_xticklabels(), coin_colors, strict=False):
    label.set_color(color)

# Add labels and title
plt.title("Distribution of Communities per Coin", fontsize=14)
plt.xlabel("Coin", fontsize=12)
plt.ylabel("Community Size (log scale)", fontsize=12)
plt.yscale("log")
plt.grid(axis="y", linestyle="--", alpha=0.7)

# Show the plot
plt.show()
```

The distribution of the communities for fraud coins tend have a lower variance, but they are also smaller graphs as there were less collected comments.

```{python}
# Count the number of communities per coin
community_counts = community_sizes_df.groupby("coin")["community_id"].nunique()

# Create a bar plot
plt.figure(figsize=(10, 6))
bars = plt.bar(
    community_counts.index,
    community_counts.values,
    color=[
        "red" if coin in fraud_coins else "darkblue"
        for coin in community_counts.index
    ],
)

# Add labels and title
plt.title("Number of Communities per Coin", fontsize=14)
plt.xlabel("Coin", fontsize=12)
plt.ylabel("Number of Communities", fontsize=12)
plt.grid(axis="y", linestyle="--", alpha=0.7)

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show the plot
plt.show()
```

The fraudulent coins have less communities, but they have less scraped comments.

```{python}
# Create a bar plot
plt.figure(figsize=(10, 6))
bars = plt.bar(
    modularties.keys(),
    modularties.values(),
    color=[
        "red" if coin in fraud_coins else "darkblue" for coin in modularties
    ],
)

# Add labels and title
plt.title("Modularity Score per Coin", fontsize=14)
plt.xlabel("Coin", fontsize=12)
plt.ylabel("Modularity", fontsize=12)
plt.grid(axis="y", linestyle="--", alpha=0.7)

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show the plot
plt.show()
```

All coins have significant modularity (`>0.3`). There is no clear difference between fraudulent and non-fraudulent coins.

```{python}
# Create a bar plot
plt.figure(figsize=(10, 6))
bars = plt.bar(
    densities.keys(),
    densities.values(),
    color=["red" if coin in fraud_coins else "darkblue" for coin in densities],
)

# Add labels and title
plt.title("Density per Coin", fontsize=14)
plt.xlabel("Coin", fontsize=12)
plt.ylabel("Density", fontsize=12)
plt.grid(axis="y", linestyle="--", alpha=0.7)

# Rotate x-axis labels for better readability
plt.xticks(rotation=45)

# Show the plot
plt.show()
```

```{python}
# Convert the data into a DataFrame for plotting
rows = []
for coin, degree_centralities_ in degree_centralities.items():
    for user, degree_centrality in degree_centralities_.items():
        rows.append(
            {
                "coin": coin,
                "user": user,
                "degree_centrality": degree_centrality,
            },
        )
degree_centralities_df = pd.DataFrame(rows)

# Prepare data for the boxplot
boxplot_data = [
    degree_centralities_df[degree_centralities_df["coin"] == coin][
        "degree_centrality"
    ]
    for coin in degree_centralities_df["coin"].unique()
]

# Create the boxplot
plt.figure(figsize=(10, 6))
box = plt.boxplot(
    boxplot_data,
    tick_labels=degree_centralities_df["coin"].unique(),
    patch_artist=True,
)

# Color the x-axis labels
ax = plt.gca()
for label, color in zip(ax.get_xticklabels(), coin_colors, strict=False):
    label.set_color(color)

# Add labels and title
plt.title("Distribution of Degree Centralities per Coin", fontsize=14)
plt.xlabel("Coin", fontsize=12)
plt.ylabel("Degree Centrality (log scale)", fontsize=12)
plt.yscale("log")
plt.ylim(0, 1)
plt.grid(axis="y", linestyle="--", alpha=0.7)

# Show the plot
plt.show()
```

```{python}
def calculate_connected_components(graph):
    """Calculate the connected components of a graph.

    If the graph is directed, it is first converted to an undirected graph.

    Parameters
    ----------
        graph (networkx.Graph): The input graph.

    Returns
    -------
        list: A list of sets, where each set contains the nodes in a connected component.

    """
    # Convert to undirected graph if necessary
    if isinstance(graph, nx.DiGraph):
        graph = graph.to_undirected()

    # Calculate connected components
    components = list(nx.connected_components(graph))
    return components


connected_components = {}
for coin, graph in graphs.items():
    connected_components[coin] = calculate_connected_components(graph)

# Count the number of connected components per coin
component_counts = {
    coin: len(components) for coin, components in connected_components.items()
}

# Create a bar plot
plt.figure(figsize=(10, 6))
bars = plt.bar(
    component_counts.keys(),
    component_counts.values(),
    color=[
        "red" if coin in fraud_coins else "darkblue"
        for coin in component_counts
    ],
)


plt.title("Number of Connected Components per Coin", fontsize=14)
plt.xlabel("Coin", fontsize=12)
plt.ylabel("Number of Connected Components", fontsize=12)
plt.grid(axis="y", linestyle="--", alpha=0.7)
plt.xticks(rotation=45)
plt.show()
```

```{python}
def calculate_diameter(graph):
    """Calculate the diameter of a graph.

    If the graph is directed, it is first converted to an undirected graph.
    For disconnected graphs, it calculates the diameter of the largest connected component.

    Parameters
    ----------
        graph (networkx.Graph): The input graph.

    Returns
    -------
        int: The diameter of the graph.

    """
    # Convert to undirected graph if necessary
    if isinstance(graph, nx.DiGraph):
        graph = graph.to_undirected()

    # Handle disconnected graphs
    if not nx.is_connected(graph):
        # Get the largest connected component
        largest_component = max(nx.connected_components(graph), key=len)
        subgraph = graph.subgraph(largest_component)
    else:
        subgraph = graph

    # Calculate and return the diameter
    return nx.diameter(subgraph)

# TODO: to slow!!!
# graph_diameters = {}
# for coin, graph in tqdm(graphs.items()):
#     graph_diameters[coin] = calculate_diameter(graph)
```

```{python}
graph_diameters
```

## Hypothesis Testing

1. The fraudulent coins have a higher relative number of moon/pump comments.
2. The fraudulent coins have a higher scores on average.


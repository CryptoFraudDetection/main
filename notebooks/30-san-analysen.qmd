---
title: Setup
jupyter: python3
---

```{python}
import numpy as np
import altair as alt
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
import os
import random
import re
from tqdm.notebook import tqdm
from collections import Counter
from community import community_louvain
from networkx.algorithms.community import label_propagation_communities
from networkx.algorithms.community.quality import modularity


alt.themes.enable("dark")
```

Read data from parquet file:

```{python}
df = pd.read_parquet("../data/processed/reddit.parquet")
datetime_columns = ['edited', 'created']
df[datetime_columns] = df[datetime_columns].apply(pd.to_datetime, errors='coerce')
```

## EDA

### Data Overview

Glimpse of the data:

```{python}
df.sample(5, random_state=42)
```

Some rows are comments and others are posts: Posts `depth` is set to -1 and they are missing `parent_id` as they initiate a potential discussion. Comments have `parent_id` set to the `id` of the post they are replying to and they are missing `url`, `num_comments`, `title` as they are not posts. Comments can be replies to posts or other comments and therefore a nested structure can be formed.

```{python}
df.dtypes
```

These are the columns in the data. Our Network will be an OneMode Network with `author` nodes and the directed edges being the comments (`depth>=0`).

#### Crypto Coins Frequency

How many posts are there for each coin?

```{python}
df.query("depth == -1")["search_query"].value_counts()
```

Some coins like Teddy Doge and BeerCoin have only a few posts. How many comments are there for each coin?

```{python}
df.query("depth > -1")["search_query"].value_counts()
```

There are multiple hundreds of commeents for each coin or more. As the comments are more important to this analysis and the coins are therefore kept.

#### Score Distribution

How are the scores distributed over the posts and comments in respect to there depth (reply to reply to reply etc.)? Note that posts have `depth` set to `-1`.

```{python}
depths = range(-1, 10)  # Tiefen von -1 bis 9
scores_by_depth = [df[df["depth"] == depth]["score"].dropna() for depth in depths]

means_by_depth = [scores.mean() for scores in scores_by_depth]

# Erstelle die Boxplots
plt.figure(figsize=(12, 8))
plt.boxplot(scores_by_depth, vert=False, patch_artist=True,
            tick_labels=[f"Depth {depth}" for depth in depths],
            boxprops=dict(facecolor='lightblue', color='blue'),
            medianprops=dict(color='red', linewidth=1.5),
            flierprops=dict(marker='o', color='orange', markersize=5, alpha=0.5)
            )

# Zeichne vertikale Linien f√ºr die Mittelwerte ein
for i, mean in enumerate(means_by_depth):
    plt.axvline(x=mean, ymin=(i+0.25) / len(depths), ymax=(i+0.75) / len(depths),
                color='green', linewidth=1.5, label='Mean' if i == 0 else "")

# Plot-Details
plt.title('Boxplot of Scores by Depth (with Mean)', fontsize=14)
plt.xlabel('Score (Log scaled)', fontsize=12)
plt.ylabel('Depth', fontsize=12)
plt.xscale('log')
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.legend()
plt.show()
```

In this plot its clearly visible, that deeper nested comments usually have a lower score. Posts have extreme outliers, which is visible in the logaritmic X-axis. The Distribution of the score is right-skewed because the mean is higher than the median.

#### Comments and Posts Distributions per Coin and Subreddit

Overview of comments (answers to other people) per coin and subreddit:

```{python}
subreddit_query = (
    df.groupby(["search_query", "subreddit"])
    .size()
    .unstack()
    .fillna(0)
    .astype(int)
    .reset_index()
    .melt(id_vars="search_query")
)

c = (
    alt.Chart(subreddit_query)
    .mark_bar()
    .encode(
        x=alt.X("value:Q", title="Number of Comments", scale={"domain": [0, 152_000]}),
        y=alt.Y("search_query:O", title="Coin"),
        color=alt.Color("subreddit:N", title="Subreddit"),
        tooltip=[
            alt.Tooltip("search_query:O", title="Coin"),
            alt.Tooltip("subreddit:N", title="Subreddit"),
            alt.Tooltip("value:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Comments per Subreddit"),
    )
)


text = (
    alt.Chart(subreddit_query)
    .mark_text(align="left", dx=5, color="white")
    .transform_calculate(customtooltip="datum.value")
    .encode(
        x=alt.X("sum(value):Q", scale={"domain": [0, 152_000]}),
        y=alt.Y("search_query:O"),
        text=alt.Text("sum(value):Q"),
    )
)

c + text
```

It's clearly visible that Bitcoin is the most popular coin and CryptoCurrency is the most popular subreddit. The interest is, how fraudulent and non-fraudulent coins differ in there social media activity. Some coins (like Teddy Doge and BeerCoin) have only a few comments.

Overview of main posts per coin and subreddit:

```{python}
subreddit_query_posts = (
    df.query("depth == -1")
    .groupby(["search_query", "subreddit"])
    .size()
    .unstack()
    .fillna(0)
    .astype(int)
    .reset_index()
    .melt(id_vars="search_query")
)

c = (
    alt.Chart(subreddit_query_posts)
    .mark_bar()
    .encode(
        x=alt.X("value:Q", title="Number of Comments", scale={"domain": [0, 1_400]}),
        y=alt.Y("search_query:O", title="Coin"),
        color=alt.Color("subreddit:N", title="Subreddit"),
        tooltip=[
            alt.Tooltip("search_query:O", title="Coin"),
            alt.Tooltip("subreddit:N", title="Subreddit"),
            alt.Tooltip("value:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Posts per Subreddit"),
    )
)


text = (
    alt.Chart(subreddit_query_posts)
    .mark_text(align="left", dx=5, color="white")
    .transform_calculate(customtooltip="datum.value")
    .encode(
        x=alt.X("sum(value):Q", scale={"domain": [0, 1_400]}),
        y=alt.Y("search_query:O"),
        text=alt.Text("sum(value):Q"),
    )
)

c + text
```

The distribution of posts is similar to the distribution of comments. Teddy Doge got only a few posts. This should not be an issue, as the comments (replies to other people) will be the edges in the graph.

#### Comments Depth Distribution

How deeply nested are the comments (replies to other people)?

```{python}
depth_df = df["depth"].value_counts().sort_index()

c = (
    alt.Chart(depth_df.reset_index())
    .mark_bar()
    .encode(
        x=alt.X("depth:O", title="Depth (-1 is the original post and 0 are top-level comments)"),
        y=alt.Y("count:Q", title="Number of Comments", scale={"domain": [0, 240_000]}),
        tooltip=[
            alt.Tooltip("depth:O", title="Depth"),
            alt.Tooltip("count:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Comments per Depth"),
    )
)

text = (
    alt.Chart(depth_df.reset_index())
    .mark_text(align="center", dy=-8, color="white")
    .transform_calculate(customtooltip="datum.count")
    .encode(
        x=alt.X("depth:O"),
        y=alt.Y("count:Q", scale={"domain": [0, 240_000]}),
        text=alt.Text("count:Q"),
    )
)

c + text
```

There are ~6k posts and most comments (~200k) are direct replies to the post. The distribution decreases ~exponentially and goes to 10 levels deep.

#### Comment/Post Count Distributions per User

How are the number of comments distributed per user?

```{python}
comments_per_user = df.groupby("author").size()
distribution = comments_per_user.value_counts().sort_index()
distribution_df = distribution.reset_index()
distribution_df.columns = ["comments_per_user", "number_of_users"]

c = (
    alt.Chart(distribution_df)
    .mark_bar()
    .encode(
        x=alt.X(
            "comments_per_user:O",
            title="Number of Comments per User (cut off at 100)",
            scale={"domain": range(1, 101)},
        ),
        y=alt.Y(
            "number_of_users:Q",
            title="Number of Users",
            scale=alt.Scale(
                type="symlog",
                domain=[0, 100_000],
            ),
            axis=alt.Axis(
                values=[
                    0,
                    1,
                    2,
                    5,
                    10,
                    20,
                    50,
                    100,
                    200,
                    500,
                    1_000,
                    2_000,
                    5_000,
                    10_000,
                    20_000,
                    50_000,
                    100_000,
                ]
            ),
        ),
        tooltip=[
            alt.Tooltip("comments_per_user:Q", title="# of Comments per User"),
            alt.Tooltip("number_of_users:Q", title="# of Users"),
        ],
    )
    .properties(
        width=1000,
        height=400,
        title=alt.Title(text="Number of Comments per User"),
    )
)

c
```

Most users have only a few comments (~70k have only one comment). There are some users with a lot of comments (cut off at 100). How extreme are the top users?

```{python}
comments_per_user.sort_values(ascending=False).head(10)
```

Some users have a lot of comments but they have `bot`, `auto` or `mod` in their name, they are therefore probably no human. They might be official bots from the subreddit for moderation purposes.

How the posts are distributed over the users:

```{python}
posts_per_user = df.query("depth == -1").groupby("author").size()
distribution = posts_per_user.value_counts().sort_index()
distribution_df = distribution.reset_index()
distribution_df.columns = ["posts_per_user", "number_of_users"]

c = (
    alt.Chart(distribution_df)
    .mark_bar()
    .encode(
        x=alt.X(
            "posts_per_user:O",
            title="Number of Posts per User (cut off at 100)",
            scale={"domain": range(1, 101)},
        ),
        y=alt.Y(
            "number_of_users:Q",
            title="Number of Users",
            scale=alt.Scale(
                type="symlog",
                domain=[0, 5_000],
                zero=True,
            ),
            axis=alt.Axis(
                values=[
                    0,
                    1,
                    2,
                    5,
                    10,
                    20,
                    50,
                    100,
                    200,
                    500,
                    1_000,
                    2_000,
                    5_000,
                ]
            ),
        ),
        tooltip=[
            alt.Tooltip("posts_per_user:Q", title="# of Posts per User"),
            alt.Tooltip("number_of_users:Q", title="# of Users"),
        ],
    )
    .properties(
        width=1000,
        height=400,
        title=alt.Title(text="Number of Posts per User"),
    )
)

c
```

The posts are distributed unevenly as well. ~3.6k users have only one post. How extreme are the top users?

```{python}
posts_per_user.sort_values(ascending=False).head(10)
```

Some users overlap with the top commenters. Examples: `kirtash93`, `goldyluckinblokchain`

#### Comment/Post Distributions over Time

How are the comments and posts distributed over time?

```{python}
posts_per_coin_per_date = df.copy()
posts_per_coin_per_date["created"] = (
    pd.to_datetime(posts_per_coin_per_date["created"])
    .dt.to_period("M")
    .dt.to_timestamp()
)

posts_per_coin_per_date = posts_per_coin_per_date.query(
    "created >= '2020-01-01'"
).query("depth >= 0")

posts_per_coin_per_date = (
    posts_per_coin_per_date.groupby(["search_query", "created"])["id"]
    .count()
    .reset_index()
    .rename(columns={"id": "number_of_comments"})
)

posts_per_coin_per_date["number_of_comments_rel"] = posts_per_coin_per_date[
    "number_of_comments"
] / posts_per_coin_per_date.groupby("search_query")["number_of_comments"].transform("sum")

step = 40
overlap = 1

c = (
    alt.Chart(posts_per_coin_per_date, height=step)
    .mark_area(
        interpolate="monotone",
        fillOpacity=0.5,
        stroke="lightgray",
        strokeWidth=0.5,
    )
    .encode(
        x=alt.X("created:T").title("Date").axis(grid=False),
        y=alt.Y("number_of_comments_rel:Q")
        .axis(None)
        .scale(range=[step, -step * overlap]),
        color=alt.Color("search_query:N", legend=None),
        tooltip=[
            alt.Tooltip("yearmonth(created):T", title="Date"),
            alt.Tooltip("search_query:N", title="Coin"),
            alt.Tooltip("number_of_comments:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=step,
    )
    .facet(
        row=alt.Row("search_query:N")
        .title(None)
        .header(labelAngle=0, labelAlign="left")
    )
    .properties(
        title=alt.Title(
            text="Number of Comments per Coin per Date after 2020",
            anchor="middle",
        ),
        bounds="flush",
    )
    .configure_facet(spacing=0)
    .configure_view(stroke=None)
    .configure_title(anchor="end")
)

c
```

```{python}
posts_per_coin_per_date = df.copy()
posts_per_coin_per_date["created"] = (
    pd.to_datetime(posts_per_coin_per_date["created"])
    .dt.to_period("M")
    .dt.to_timestamp()
)

posts_per_coin_per_date = posts_per_coin_per_date.query(
    "created >= '2020-01-01'"
).query("depth == -1")


posts_per_coin_per_date = (
    posts_per_coin_per_date.groupby(["search_query", "created"])["id"]
    .count()
    .reset_index()
    .rename(columns={"id": "number_of_posts"})
)

posts_per_coin_per_date["number_of_posts_rel"] = posts_per_coin_per_date[
    "number_of_posts"
] / posts_per_coin_per_date.groupby("search_query")["number_of_posts"].transform("sum")

posts_per_coin_per_date

step = 40
overlap = 1

c = (
    alt.Chart(posts_per_coin_per_date, height=step)
    .mark_area(
        interpolate="monotone",
        fillOpacity=0.5,
        stroke="lightgray",
        strokeWidth=0.5,
    )
    .encode(
        x=alt.X("created:T").title("Date").axis(grid=False),
        y=alt.Y("number_of_posts_rel:Q")
        .axis(None)
        .scale(range=[step, -step * overlap]),
        color=alt.Color("search_query:N", legend=None),
        tooltip=[
            alt.Tooltip("yearmonth(created):T", title="Date"),
            alt.Tooltip("search_query:N", title="Coin"),
            alt.Tooltip("number_of_posts:Q", title="# of Posts"),
        ],
    )
    .properties(
        width=800,
        height=step,
    )
    .facet(
        row=alt.Row("search_query:N")
        .title(None)
        .header(labelAngle=0, labelAlign="left")
    )
    .properties(
        title=alt.Title(
            text="Number of Posts per Coin per Date after 2020",
            anchor="middle",
        ),
        bounds="flush",
    )
    .configure_facet(spacing=0)
    .configure_view(stroke=None)
    .configure_title(anchor="end")
)

c
```

The plots are cut off at 2020 because there are not many posts or comments before that time (in relation to the rest of the data). Some coins have more data at the end of the timescale because we used google results to get the post URLs and google had probably some bais towards newer posts.

### NA Values

#### Posts

How many posts are there?

```{python}
print('Number of posts:', len(df.query("depth == -1")))
```

`parent_id` is expeted to be NA for all posts. Columns with missing values for posts:

```{python}
df.query("depth == -1").isna().sum()[lambda x: x > 0]
```

Some posts have been edited but this is ignored in this analysis.

#### Comments

How many comments are there?

```{python}
print('Number of comments:', len(df[df["depth"] >= 0]))
```

`title`, `url` and `num_comments` are not used for comments and are expected to be NA. Which columns do have missing values for comments?

```{python}
df.query("depth >= 0").isna().sum()[lambda x: x > 0]
```

`edited` is ignored for comments as well.

Posts are not of primary interest for this analysis and therefore unnecessary columns are dropped (even though they are not always NA for posts):

```{python}
columns = ["edited", "title", "url", "num_comments"]
df.drop(columns=columns, inplace=True)
print("Dropped columns:")
print(columns)
print("Columns remaining:")
print(df.columns.to_list())
```

## User Interaction Network

In this analysis the interactions between users is analyzed and therefore a column `paprent_user` is created with the `author` of the post/comment the user is replying to in this comment. This enables the creation of a network with users as nodes and directed edges between the users being the comments. The first network might have multiple edges between the same nodes if the users have replied to each other multiple times. A second network is created with only one edge between the same nodes, the edge weight being the number of comments between the two users. Edges get critical word hit counts as edge attributes.

```{python}
df = df.merge(
    df[["id", "author"]].rename(columns={"id": "parent_id", "author": "parent_user"}),
    on="parent_id",
    how="left"
)

patterns = {"moon": r"moon", "pump": r"pump"}
for col, pattern in patterns.items():
    df[col] = df["body"].str.contains(pattern, flags=re.IGNORECASE, na=False)
```

```{python}
comments_df = df.query("depth >= 0")


G = nx.DiGraph()
G.add_nodes_from(comments_df["author"].unique())

id_to_author = dict(zip(comments_df["id"], comments_df["author"]))

# F√ºge Kanten hinzu
for _, row in comments_df.iterrows():
    parent_id = row["parent_id"]
    author = row["author"]

    # √úberpr√ºfen, ob der parent_id ein bekannter Knoten ist
    if parent_id in id_to_author:
        parent_author = id_to_author[parent_id]
        G.add_edge(parent_author, author)

# Graph speichern oder visualisieren
output_path = "../data/processed/comments_network.gexf"
nx.write_gexf(G, output_path)
print(f"Graph gespeichert unter: {output_path}")
```

```{python}
node_degrees = dict(G.degree())

# Sortiere die Knoten nach ihrer Degree absteigend
top_nodes = sorted(node_degrees, key=node_degrees.get, reverse=True)[:20]

# Erstelle einen Subgraphen mit den Top 20 Knoten
top_subgraph = G.subgraph(top_nodes)

# Zeichne den Subgraphen
plt.figure(figsize=(10, 8))
pos = nx.spring_layout(top_subgraph, seed=42)  # Schnelles Layout
nx.draw_networkx_nodes(top_subgraph, pos, node_size=50, alpha=0.7)
nx.draw_networkx_edges(top_subgraph, pos, alpha=0.3, edge_color="gray")

# F√ºge Labels (Usernames) hinzu
labels = {node: node for node in top_subgraph.nodes}
nx.draw_networkx_labels(top_subgraph, pos, labels, font_size=10, font_color="black", alpha=0.9)

# Titel und Anzeige
plt.title("Network of Top 20 Users (most Edges)", fontsize=14)
plt.axis("off")
plt.show()
```

Here we can see the top 20 Users with the most comments. They are heavily interconnected. 

TODO: what could we do else in eda?

## Analysis

Are there Users, which answer often to the same users?

```{python}
user_parent_counts = df.groupby(["parent_user", "author"]).size().reset_index(name="count")

top_user_parent = user_parent_counts.sort_values(by="count", ascending=False).head(10)

print("Users with the most interactions with the same parent user:")
print(top_user_parent)
```

These are the User with most comments to the same user. There are many Moderator bots in the list and also None.

```{python}
comments_df = df[df["score"] > 0]

# Sort by created (assuming created is a timestamp or sortable column) and take the newest 1000 per search_query
comments_df = (
    comments_df.sort_values(by="created", ascending=False)
    .groupby("search_query")
    .head(500)
)

print(len(comments_df))
# Group by parent_user and author, summing up the scores
user_edges = (
    comments_df.groupby(["parent_user", "author"])["score"].sum().reset_index()
)



# Add 1 to the summed scores for the edge weight
user_edges["weight"] = user_edges["score"] + 1

# Create a directed graph
G_weighted = nx.DiGraph()

# Add edges with calculated weights and search_query as an edge attribute
for _, row in tqdm(comments_df.iterrows(), desc="Building Graph"):
    G_weighted.add_edge(
        row["parent_user"], 
        row["author"], 
        weight=row["score"] + 1, 
        search_query=row["search_query"],
        moon=row["moon"],  # Add 'moon' as an edge attribute
        pump=row["pump"]   # Add 'pump' as an edge attribute
    )
```

```{python}
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_weighted, seed=42, k=60, scale=3.0)

# Draw nodes and edges
nx.draw_networkx_nodes(G_weighted, pos, node_size=1, alpha=0.7)
nx.draw_networkx_edges(G_weighted, pos, width=1, alpha=0.5, arrowstyle="->", arrowsize=8)

edge_labels = nx.get_edge_attributes(G_weighted, "weight")
for (u, v), label in edge_labels.items():
    x, y = (pos[u][0] + pos[v][0]) / 2, (pos[u][1] + pos[v][1]) / 2  # Midpoint of edge
    plt.text(x, y, f"{label:.1f}", fontsize=6, color="black", alpha=0.9)

plt.title(
    "User Interaction Graph with Weighted Edges (Filtered by Score > 0 and Newest 500 per Search Query)",
    fontsize=16,
)
plt.axis("off")
plt.show()
```

```{python}
G_undirected = G_weighted.to_undirected()

# Perform Label Propagation Clustering on the undirected graph
tqdm_desc = "Label Propagation Clustering"
with tqdm(total=1, desc=tqdm_desc) as pbar:
    communities = list(label_propagation_communities(G_undirected))
    pbar.update(1)

# Assign communities to nodes
node_community_mapping = {node: i for i, community in enumerate(communities) for node in community}
for node, community in node_community_mapping.items():
    G_undirected.nodes[node]['community'] = community

# Display the number of clusters
num_clusters = len(communities)
print(f"Label Propagation Clustering completed: {num_clusters} clusters found.")
```

```{python}
#print size of clusters
cluster_sizes = Counter(node_community_mapping.values())
#sort
cluster_sizes = dict(sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True))
print("Size of clusters:")
print(cluster_sizes)
```

```{python}
self_edges = [(u, v) for u, v in G_undirected.edges() if u == v]
non_self_edges = [(u, v) for u, v in G_undirected.edges() if u != v]

# Create subgraphs for self-references and non-self-references
G_self = G_undirected.edge_subgraph(self_edges).copy()
G_non_self = G_undirected.edge_subgraph(non_self_edges).copy()

# Set up subplots
fig, axes = plt.subplots(1, 2, figsize=(20, 10))
fig.suptitle("Self-references vs Non-self-references in User Interaction Graph (with Clusters)", fontsize=18)

# Plot self-references with clusters
ax = axes[0]
pos_self = nx.spring_layout(G_self, seed=42, k=0.1)
community_colors_self = [G_self.nodes[node]['community'] for node in G_self.nodes()]
nx.draw_networkx_nodes(G_self, pos_self, node_size=50, alpha=0.8, ax=ax, node_color=community_colors_self, cmap=plt.cm.Set3)
nx.draw_networkx_edges(G_self, pos_self, width=1.5, alpha=0.6, ax=ax)
ax.set_title("Self-references (Edges on Themselves)", fontsize=16)
ax.axis("off")

# Plot non-self-references with clusters
ax = axes[1]
pos_non_self = nx.spring_layout(G_non_self, seed=42)
community_colors_non_self = [G_non_self.nodes[node]['community'] for node in G_non_self.nodes()]
nx.draw_networkx_nodes(G_non_self, pos_non_self, node_size=50, alpha=0.8, ax=ax, node_color=community_colors_non_self, cmap=plt.cm.Set3)
nx.draw_networkx_edges(G_non_self, pos_non_self, width=1.5, alpha=0.6, ax=ax)
ax.set_title("Non-self-references (Edges with No Self-references)", fontsize=16)
ax.axis("off")

plt.show()
```

```{python}
cluster_sizes = Counter(nx.get_node_attributes(G_undirected, 'community').values())

# Filter for clusters greater than size 2
valid_clusters = {cluster for cluster, size in cluster_sizes.items() if size > 10}

# Filter nodes belonging to valid clusters
nodes_in_valid_clusters = [node for node in G_undirected.nodes if G_undirected.nodes[node]['community'] in valid_clusters]

# Create a subgraph with only the valid clusters
G_filtered = G_undirected.subgraph(nodes_in_valid_clusters).copy()

# Plot the filtered graph
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_filtered, k=0.1, seed=42)

# Draw nodes and edges
community_colors = [G_filtered.nodes[node]['community'] for node in G_filtered.nodes()]
nx.draw_networkx_nodes(G_filtered, pos, node_size=10, alpha=0.8, node_color=community_colors, cmap=plt.cm.Set3)
nx.draw_networkx_edges(G_filtered, pos, width=1.5, alpha=0.6)

plt.title("Filtered User Interaction Graph (Clusters with Size > 2)", fontsize=16)
plt.axis("off")
plt.show()
```

```{python}
node_search_queries = {node: [] for node in G_undirected.nodes}

# Collect search queries for each node based on connected edges
for u, v, data in G_undirected.edges(data=True):
    if "search_query" in data:
        node_search_queries[u].append(data["search_query"])
        node_search_queries[v].append(data["search_query"])

# Assign the most common 'search_query' to each node
for node, queries in node_search_queries.items():
    most_common_query = Counter(queries).most_common(1)
    G_undirected.nodes[node]["search_query"] = most_common_query[0][0] if most_common_query else "Unknown"

# Identify clusters with size > 10
cluster_sizes = Counter(nx.get_node_attributes(G_undirected, "community").values())
valid_clusters = {cluster for cluster, size in cluster_sizes.items() if size > 10}

# Filter nodes belonging to valid clusters
nodes_in_valid_clusters = [
    node for node in G_undirected.nodes 
    if G_undirected.nodes[node]["community"] in valid_clusters
]

# Create a subgraph with only the valid clusters
G_filtered = G_undirected.subgraph(nodes_in_valid_clusters).copy()

# Assign colors based on 'search_query'
search_query_colors = nx.get_node_attributes(G_filtered, "search_query")
unique_queries = list(set(search_query_colors.values()))
color_map = {query: idx for idx, query in enumerate(unique_queries)}
color_values = [color_map[search_query_colors[node]] for node in G_filtered.nodes()]

# Plot the filtered graph
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_filtered, k=0.1)

nx.draw_networkx_nodes(G_filtered, pos, node_size=10, alpha=0.8, node_color=color_values, cmap=plt.cm.tab20)
nx.draw_networkx_edges(G_filtered, pos, width=1.5, alpha=0.6)

plt.title("Filtered User Interaction Graph (Clusters with Size > 10, Colored by Search Query)", fontsize=16)
plt.axis("off")
plt.show()
```

```{python}
search_queries = set(nx.get_edge_attributes(G_filtered, "search_query").values())

# Create and plot a subgraph for each search query
for search_query in search_queries:
    # Filter edges with the current search query
    edges_for_query = [
        (u, v) for u, v, data in G_filtered.edges(data=True) if data["search_query"] == search_query
    ]

    # Create a subgraph for the current search query
    G_query = G_filtered.edge_subgraph(edges_for_query).copy()

    # Get cluster colors for the nodes
    cluster_colors = [G_query.nodes[node]["community"] for node in G_query.nodes()]
    unique_clusters = list(set(cluster_colors))
    cluster_color_map = {cluster: idx for idx, cluster in enumerate(unique_clusters)}
    node_colors = [cluster_color_map[G_query.nodes[node]["community"]] for node in G_query.nodes()]

    # Plot the graph
    plt.figure(figsize=(12, 10))
    pos = nx.spring_layout(G_query, k=0.1)

    nx.draw_networkx_nodes(G_query, pos, node_size=10, alpha=0.8, node_color=node_colors, cmap=plt.cm.tab20)
    nx.draw_networkx_edges(G_query, pos, width=1.5, alpha=0.6)

    plt.title(f"User Interaction Graph for Search Query: {search_query}", fontsize=16)
    plt.axis("off")
    plt.show()
```

```{python}
search_queries = set(nx.get_edge_attributes(G_filtered, "search_query").values())

# Create and plot a subgraph for each search query with modularity labels
for search_query in search_queries:
    # Filter edges with the current search query
    edges_for_query = [
        (u, v) for u, v, data in G_filtered.edges(data=True) if data["search_query"] == search_query
    ]

    # Create a subgraph for the current search query
    G_query = G_filtered.edge_subgraph(edges_for_query).copy()

    # Identify communities
    communities = {}
    for node in G_query.nodes:
        community = G_query.nodes[node]["community"]
        if community not in communities:
            communities[community] = set()
        communities[community].add(node)

    # Calculate modularity
    modularity_value = modularity(G_query, communities.values())

    # Get cluster colors for the nodes
    cluster_colors = [G_query.nodes[node]["community"] for node in G_query.nodes()]
    unique_clusters = list(set(cluster_colors))
    cluster_color_map = {cluster: idx for idx, cluster in enumerate(unique_clusters)}
    node_colors = [cluster_color_map[G_query.nodes[node]["community"]] for node in G_query.nodes()]

    # Plot the graph
    plt.figure(figsize=(12, 10))
    pos = nx.spring_layout(G_query, k=0.1)

    nx.draw_networkx_nodes(G_query, pos, node_size=10, alpha=0.8, node_color=node_colors, cmap=plt.cm.tab20)
    nx.draw_networkx_edges(G_query, pos, width=1.5, alpha=0.6)

    # Add modularity label
    plt.text(
        0.5, 0.95, f"Modularity: {modularity_value:.3f}", fontsize=14, color="black",
        horizontalalignment='center', transform=plt.gca().transAxes
    )

    plt.title(f"User Interaction Graph for Search Query: {search_query}", fontsize=16)
    plt.axis("off")
    plt.show()
```

```{python}
def calculate_internal_density(cluster_nodes, graph):
    subgraph = graph.subgraph(cluster_nodes)
    num_edges = subgraph.number_of_edges()
    num_nodes = subgraph.number_of_nodes()
    if num_nodes <= 1:
        return 0  # Avoid division by zero
    possible_edges = num_nodes * (num_nodes - 1) / 2
    return num_edges / possible_edges if possible_edges > 0 else 0

# Get unique search queries from the graph
search_queries = set(nx.get_edge_attributes(G_filtered, "search_query").values())

# Create and plot a subgraph for each search query with internal density labels
for search_query in search_queries:
    # Filter edges with the current search query
    edges_for_query = [
        (u, v) for u, v, data in G_filtered.edges(data=True) if data["search_query"] == search_query
    ]

    # Create a subgraph for the current search query
    G_query = G_filtered.edge_subgraph(edges_for_query).copy()

    # Identify communities
    communities = {}
    for node in G_query.nodes:
        community = G_query.nodes[node]["community"]
        if community not in communities:
            communities[community] = set()
        communities[community].add(node)

    # Calculate internal density for each community
    community_densities = {}
    for community, nodes in communities.items():
        community_densities[community] = calculate_internal_density(nodes, G_query)

    # Get cluster colors for the nodes
    cluster_colors = [G_query.nodes[node]["community"] for node in G_query.nodes()]
    unique_clusters = list(set(cluster_colors))
    cluster_color_map = {cluster: idx for idx, cluster in enumerate(unique_clusters)}
    node_colors = [cluster_color_map[G_query.nodes[node]["community"]] for node in G_query.nodes()]

    # Plot the graph
    plt.figure(figsize=(12, 10))
    pos = nx.spring_layout(G_query, k=0.1)

    nx.draw_networkx_nodes(G_query, pos, node_size=10, alpha=0.8, node_color=node_colors, cmap=plt.cm.tab20)
    nx.draw_networkx_edges(G_query, pos, width=1.5, alpha=0.6)

    # Add internal density labels for each cluster
    for community, density in community_densities.items():
        cluster_nodes = communities[community]
        cluster_center = [pos[node] for node in cluster_nodes]
        center_x = sum([coord[0] for coord in cluster_center]) / len(cluster_center)
        center_y = sum([coord[1] for coord in cluster_center]) / len(cluster_center)
        plt.text(
            center_x, center_y, f"{density:.3f}", fontsize=10, color="black", 
            horizontalalignment='center', bbox=dict(facecolor="white", alpha=0.5, boxstyle="round,pad=0.3")
        )

    plt.title(f"User Interaction Graph for Search Query: {search_query} (Internal Density)", fontsize=16)
    plt.axis("off")
    plt.show()
```

```{python}
search_queries = set(nx.get_edge_attributes(G_filtered, "search_query").values())
modularity_values = []

for search_query in search_queries:
    # Filter edges with the current search query
    edges_for_query = [
        (u, v) for u, v, data in G_filtered.edges(data=True) if data["search_query"] == search_query
    ]

    # Create a subgraph for the current search query
    G_query = G_filtered.edge_subgraph(edges_for_query).copy()

    # Identify communities
    communities = {}
    for node in G_query.nodes:
        community = G_query.nodes[node]["community"]
        if community not in communities:
            communities[community] = set()
        communities[community].add(node)

    # Calculate modularity
    modularity_value = modularity(G_query, communities.values())
    modularity_values.append((search_query, modularity_value))

# Sort the modularity values for better visualization
modularity_values.sort(key=lambda x: x[1], reverse=True)

# Extract data for plotting
queries = [item[0] for item in modularity_values]
modularity_scores = [item[1] for item in modularity_values]

# Create the barplot
plt.figure(figsize=(12, 8))
plt.barh(queries, modularity_scores, color='skyblue', edgecolor='black')
plt.xlabel("Modularity", fontsize=14)
plt.ylabel("Search Query", fontsize=14)
plt.title("Modularity Scores for Each Search Query", fontsize=16)
plt.gca().invert_yaxis()  # Invert y-axis for better readability
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()
```


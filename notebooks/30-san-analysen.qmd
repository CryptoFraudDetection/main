---
title: Setup
jupyter: python3
---

```{python}
import numpy as np
import altair as alt
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
import os
import random
from tqdm.notebook import tqdm
from collections import Counter
from community import community_louvain
from networkx.algorithms.community import label_propagation_communities
from networkx.algorithms.community.quality import modularity


alt.themes.enable("dark")
```

Read data from parquet file:

```{python}
df = pd.read_parquet("../data/processed/reddit.parquet")
```

## EDA

Glimpse of the data:

```{python}
df.sample(5, random_state=42)
```

```{python}
#describe df
df.dtypes
```

These are the columns in the data. Our Network will be an OneMode Network with the nodes being the `author` and the directed edges being the comments (depth >=0).

```{python}
#na in columns of posts (depth = -1)
df[df["depth"] == -1].isna().sum()
```

All NA's in parent_id are there, becuase we filtered for posts and posts don't have a parent_id.

```{python}
#na in columns of posts (depth >= 0)
print("na in columns of posts (depth >= 0)")
display( df[df["depth"] >= 0].isna().sum())
print('len of comments with depth >= 0')
display(len(df[df["depth"] >= 0]))
```

All NA's are in columns which we dont need. Thats why we will delete these columns.

```{python}
#delete columns edited, title, url, num_comments, subreddit
df.drop(columns=["edited", "title", "url", "num_comments"], inplace=True)
```

These columns get deleted: "edited", "title", "url", "num_comments", because we dont need them. 

Which coin appears how often in the data?

```{python}
df["search_query"].value_counts()
```

```{python}
id_to_author = dict(zip(df["id"], df["author"]))

df["parent_user"] = df["parent_id"].map(id_to_author)
df.head()
```

Here we added a new column, parent_user, which is the author of the parent_id. This is important for the network, because we want to know who commented on who.

```{python}
#new column if word 'moon' nad one if word 'pump' in body
df["moon"] = df["body"].str.contains("moon", case=False)
df["pump"] = df["body"].str.contains("pump", case=False)
```

```{python}
depths = range(-1, 10)  # Tiefen von -1 bis 9
scores_by_depth = [df[df["depth"] == depth]["score"].dropna() for depth in depths]

means_by_depth = [scores.mean() for scores in scores_by_depth]

# Erstelle die Boxplots
plt.figure(figsize=(12, 8))
plt.boxplot(scores_by_depth, vert=False, patch_artist=True,
            labels=[f"Depth {depth}" for depth in depths],
            boxprops=dict(facecolor='lightblue', color='blue'),
            medianprops=dict(color='red', linewidth=1.5),
            flierprops=dict(marker='o', color='orange', markersize=5, alpha=0.5)
            )

# Zeichne vertikale Linien für die Mittelwerte ein
for i, mean in enumerate(means_by_depth):
    plt.axvline(x=mean, ymin=(i+0.25) / len(depths), ymax=(i+0.75) / len(depths),
                color='green', linewidth=1.5, label='Mean' if i == 0 else "")

# Plot-Details
plt.title('Boxplot of Scores by Depth (with Mean)', fontsize=14)
plt.xlabel('Score (Log scaled)', fontsize=12)
plt.ylabel('Depth', fontsize=12)
plt.xscale('log')
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.legend()
plt.show()
```

In this plot its clearly visible, that deeper nested comments usually have a lower score. Posts have extreme outliers, which is visible in the logaritmic X-axis. The Distribution of the score is right-skewed because the mean is higher than the median.

Overview of comments (answers to other people) per coin and subreddit:

```{python}
subreddit_query = (
    df.groupby(["search_query", "subreddit"])
    .size()
    .unstack()
    .fillna(0)
    .astype(int)
    .reset_index()
    .melt(id_vars="search_query")
)

c = (
    alt.Chart(subreddit_query)
    .mark_bar()
    .encode(
        x=alt.X("value:Q", title="Number of Comments", scale={"domain": [0, 152_000]}),
        y=alt.Y("search_query:O", title="Coin"),
        color=alt.Color("subreddit:N", title="Subreddit"),
        tooltip=[
            alt.Tooltip("search_query:O", title="Coin"),
            alt.Tooltip("subreddit:N", title="Subreddit"),
            alt.Tooltip("value:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Comments per Subreddit"),
    )
)


text = (
    alt.Chart(subreddit_query)
    .mark_text(align="left", dx=5, color="white")
    .transform_calculate(customtooltip="datum.value")
    .encode(
        x=alt.X("sum(value):Q", scale={"domain": [0, 152_000]}),
        y=alt.Y("search_query:O"),
        text=alt.Text("sum(value):Q"),
    )
)

c + text
```

It's clearly visible that Bitcoin is the most popular coin and CryptoCurrency is the most popular subreddit. The interest is, how fraudulent and non-fraudulent coins differ in there social media activity. Some coins (like Teddy Doge and BeerCoin) have only a few comments.

Overview of main posts per coin and subreddit:

```{python}
subreddit_query_posts = (
    df.query("depth == -1")
    .groupby(["search_query", "subreddit"])
    .size()
    .unstack()
    .fillna(0)
    .astype(int)
    .reset_index()
    .melt(id_vars="search_query")
)

c = (
    alt.Chart(subreddit_query_posts)
    .mark_bar()
    .encode(
        x=alt.X("value:Q", title="Number of Comments", scale={"domain": [0, 1_400]}),
        y=alt.Y("search_query:O", title="Coin"),
        color=alt.Color("subreddit:N", title="Subreddit"),
        tooltip=[
            alt.Tooltip("search_query:O", title="Coin"),
            alt.Tooltip("subreddit:N", title="Subreddit"),
            alt.Tooltip("value:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Posts per Subreddit"),
    )
)


text = (
    alt.Chart(subreddit_query_posts)
    .mark_text(align="left", dx=5, color="white")
    .transform_calculate(customtooltip="datum.value")
    .encode(
        x=alt.X("sum(value):Q", scale={"domain": [0, 1_400]}),
        y=alt.Y("search_query:O"),
        text=alt.Text("sum(value):Q"),
    )
)

c + text
```

The distribution of posts is similar to the distribution of comments. Teddy Doge got only a few posts. This should not be an issue, as the comments (replies to other people) will be the edges in the graph.

How deeply nested are the comments (replies to other people)?

```{python}
depth_df = df["depth"].value_counts().sort_index()

c = (
    alt.Chart(depth_df.reset_index())
    .mark_bar()
    .encode(
        x=alt.X("depth:O", title="Depth (-1 is the original post)"),
        y=alt.Y("count:Q", title="Number of Comments", scale={"domain": [0, 240_000]}),
        tooltip=[
            alt.Tooltip("depth:O", title="Depth"),
            alt.Tooltip("count:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Comments per Depth"),
    )
)

text = (
    alt.Chart(depth_df.reset_index())
    .mark_text(align="center", dy=-8, color="white")
    .transform_calculate(customtooltip="datum.count")
    .encode(
        x=alt.X("depth:O"),
        y=alt.Y("count:Q", scale={"domain": [0, 240_000]}),
        text=alt.Text("count:Q"),
    )
)

c + text
```

There are ~6k posts and most comments (~200k) are direct replies to the post. The distribution decreases ~exponentially and goes to 10 levels deep.

How are the number of comments distributed per user?

```{python}
comments_per_user = df.groupby("author").size()
distribution = comments_per_user.value_counts().sort_index()
distribution_df = distribution.reset_index()
distribution_df.columns = ["comments_per_user", "number_of_users"]

c = (
    alt.Chart(distribution_df)
    .mark_bar()
    .encode(
        x=alt.X(
            "comments_per_user:O",
            title="Number of Comments per User (cut off at 100)",
            scale={"domain": range(1, 101)},
        ),
        y=alt.Y(
            "number_of_users:Q",
            title="Number of Users",
            scale=alt.Scale(
                type="symlog",
                domain=[0, 100_000],
            ),
            axis=alt.Axis(
                values=[
                    0,
                    1,
                    2,
                    5,
                    10,
                    20,
                    50,
                    100,
                    200,
                    500,
                    1_000,
                    2_000,
                    5_000,
                    10_000,
                    20_000,
                    50_000,
                    100_000,
                ]
            ),
        ),
        tooltip=[
            alt.Tooltip("comments_per_user:Q", title="# of Comments per User"),
            alt.Tooltip("number_of_users:Q", title="# of Users"),
        ],
    )
    .properties(
        width=1000,
        height=400,
        title=alt.Title(text="Number of Comments per User"),
    )
)

c
```

Most users have only a few comments (~70k have only one comment). There are some users with a lot of comments (cut off at 100). How extreme are the top users?

```{python}
comments_per_user.sort_values(ascending=False).head(10)
```

Some users have a lot of comments but they have `bot`, `auto` or `mod` in their name, which means that they are probably no human. They might be official bots from the subreddit for moderation purposes.

Now lets see how the posts are distributed over the users:

```{python}
posts_per_user = df.query("depth == -1").groupby("author").size()
distribution = posts_per_user.value_counts().sort_index()
distribution_df = distribution.reset_index()
distribution_df.columns = ["posts_per_user", "number_of_users"]

c = (
    alt.Chart(distribution_df)
    .mark_bar()
    .encode(
        x=alt.X(
            "posts_per_user:O",
            title="Number of Posts per User (cut off at 100)",
            scale={"domain": range(1, 101)},
        ),
        y=alt.Y(
            "number_of_users:Q",
            title="Number of Users",
            scale=alt.Scale(
                type="symlog",
                domain=[0, 5_000],
                zero=True,
            ),
            axis=alt.Axis(
                values=[
                    0,
                    1,
                    2,
                    5,
                    10,
                    20,
                    50,
                    100,
                    200,
                    500,
                    1_000,
                    2_000,
                    5_000,
                ]
            ),
        ),
        tooltip=[
            alt.Tooltip("posts_per_user:Q", title="# of Posts per User"),
            alt.Tooltip("number_of_users:Q", title="# of Users"),
        ],
    )
    .properties(
        width=1000,
        height=400,
        title=alt.Title(text="Number of Posts per User"),
    )
)

c
```

The posts are distributed unevenly as well. ~3.6k users have only one post. Let's have a look at the top users (for posts):

```{python}
posts_per_user.sort_values(ascending=False).head(10)
```

Some users overlap with the top commenters. Examples: kirtash93, goldyluckinblokchain

Let's have a look at the comment distribution over time per coin:

```{python}
posts_per_coin_per_date = df.copy()
posts_per_coin_per_date["created"] = (
    pd.to_datetime(posts_per_coin_per_date["created"])
    .dt.to_period("M")
    .dt.to_timestamp()
)

# Filter for comments with depth 0 to 9
posts_per_coin_per_date = posts_per_coin_per_date.query(
    "created >= '2020-01-01'"
).query("depth >= 0 and depth <= 9")

posts_per_coin_per_date = (
    posts_per_coin_per_date.groupby(["search_query", "created"])["id"]
    .count()
    .reset_index()
    .rename(columns={"id": "number_of_comments"})
)

posts_per_coin_per_date["number_of_comments_rel"] = posts_per_coin_per_date[
    "number_of_comments"
] / posts_per_coin_per_date.groupby("search_query")["number_of_comments"].transform("sum")

step = 40
overlap = 1

c = (
    alt.Chart(posts_per_coin_per_date, height=step)
    .mark_area(
        interpolate="monotone",
        fillOpacity=0.5,
        stroke="lightgray",
        strokeWidth=0.5,
    )
    .encode(
        x=alt.X("created:T").title("Date").axis(grid=False),
        y=alt.Y("number_of_comments_rel:Q")
        .axis(None)
        .scale(range=[step, -step * overlap]),
        color=alt.Color("search_query:N", legend=None),
        tooltip=[
            alt.Tooltip("yearmonth(created):T", title="Date"),
            alt.Tooltip("search_query:N", title="Coin"),
            alt.Tooltip("number_of_comments:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=step,
    )
    .facet(
        row=alt.Row("search_query:N")
        .title(None)
        .header(labelAngle=0, labelAlign="left")
    )
    .properties(
        title=alt.Title(
            text="Number of Comments per Coin per Date after 2020",
            anchor="middle",
        ),
        bounds="flush",
    )
    .configure_facet(spacing=0)
    .configure_view(stroke=None)
    .configure_title(anchor="end")
)

c
```

Let's have a look at the comment distribution over time per coin:

```{python}
posts_per_coin_per_date = df.copy()
posts_per_coin_per_date["created"] = (
    pd.to_datetime(posts_per_coin_per_date["created"])
    .dt.to_period("M")
    .dt.to_timestamp()
)

posts_per_coin_per_date = posts_per_coin_per_date.query(
    "created >= '2020-01-01'"
).query("depth == -1")


posts_per_coin_per_date = (
    posts_per_coin_per_date.groupby(["search_query", "created"])["id"]
    .count()
    .reset_index()
    .rename(columns={"id": "number_of_posts"})
)

posts_per_coin_per_date["number_of_posts_rel"] = posts_per_coin_per_date[
    "number_of_posts"
] / posts_per_coin_per_date.groupby("search_query")["number_of_posts"].transform("sum")

posts_per_coin_per_date

step = 40
overlap = 1

c = (
    alt.Chart(posts_per_coin_per_date, height=step)
    .mark_area(
        interpolate="monotone",
        fillOpacity=0.5,
        stroke="lightgray",
        strokeWidth=0.5,
    )
    .encode(
        x=alt.X("created:T").title("Date").axis(grid=False),
        y=alt.Y("number_of_posts_rel:Q")
        .axis(None)
        .scale(range=[step, -step * overlap]),
        color=alt.Color("search_query:N", legend=None),
        tooltip=[
            alt.Tooltip("yearmonth(created):T", title="Date"),
            alt.Tooltip("search_query:N", title="Coin"),
            alt.Tooltip("number_of_posts:Q", title="# of Posts"),
        ],
    )
    .properties(
        width=800,
        height=step,
    )
    .facet(
        row=alt.Row("search_query:N")
        .title(None)
        .header(labelAngle=0, labelAlign="left")
    )
    .properties(
        title=alt.Title(
            text="Number of Posts per Coin per Date after 2020",
            anchor="middle",
        ),
        bounds="flush",
    )
    .configure_facet(spacing=0)
    .configure_view(stroke=None)
    .configure_title(anchor="end")
)

c
```

The plot is cut off at 2020 because there are not many posts or comments before that time (in relation to the rest of the data). Some cions have more data at the end of the timescale because we used google results to get the post URLs and google had probably some bais towards newer posts.

```{python}
filtered_df = df[df["depth"] >= 0]

# Initialisiere einen Graphen
G = nx.DiGraph()  # Direktes Netzwerk, da Kommentare gerichtet sind

# Füge Knoten (autoren) hinzu
G.add_nodes_from(filtered_df["author"].unique())

# Map von id zu author
id_to_author = dict(zip(filtered_df["id"], filtered_df["author"]))

# Füge Kanten hinzu
for _, row in filtered_df.iterrows():
    parent_id = row["parent_id"]
    author = row["author"]

    # Überprüfen, ob der parent_id ein bekannter Knoten ist
    if parent_id in id_to_author:
        parent_author = id_to_author[parent_id]
        G.add_edge(parent_author, author)

# Graph speichern oder visualisieren
output_path = "../data/processed/comments_network.gexf"
nx.write_gexf(G, output_path)
print(f"Graph gespeichert unter: {output_path}")


```

```{python}
node_degrees = dict(G.degree())

# Sortiere die Knoten nach ihrer Degree absteigend
top_nodes = sorted(node_degrees, key=node_degrees.get, reverse=True)[:20]

# Erstelle einen Subgraphen mit den Top 20 Knoten
top_subgraph = G.subgraph(top_nodes)

# Zeichne den Subgraphen
plt.figure(figsize=(10, 8))
pos = nx.spring_layout(top_subgraph, seed=42)  # Schnelles Layout
nx.draw_networkx_nodes(top_subgraph, pos, node_size=50, alpha=0.7)
nx.draw_networkx_edges(top_subgraph, pos, alpha=0.3, edge_color="gray")

# Füge Labels (Usernames) hinzu
labels = {node: node for node in top_subgraph.nodes}
nx.draw_networkx_labels(top_subgraph, pos, labels, font_size=10, font_color="black", alpha=0.9)

# Titel und Anzeige
plt.title("Network of Top 20 Users (most Edges)", fontsize=14)
plt.axis("off")
plt.show()
```


Here we can see the top 20 Users with the most comments. They are heavily interconnected. 
TODO: what could we do else in eda?

## Analysis

Are there Users, which answer often to the same users?

```{python}

user_parent_counts = df.groupby(["parent_user", "author"]).size().reset_index(name="count")

top_user_parent = user_parent_counts.sort_values(by="count", ascending=False).head(10)

print("Users with the most interactions with the same parent user:")
print(top_user_parent)
```

These are the User with most comments to the same user. There are many Moderator bots in the list and also None.

```{python}


filtered_df = df[df["score"] > 0]

# Sort by created (assuming created is a timestamp or sortable column) and take the newest 1000 per search_query
filtered_df = (
    filtered_df.sort_values(by="created", ascending=False)
    .groupby("search_query")
    .head(500)
)

print(len(filtered_df))
# Group by parent_user and author, summing up the scores
user_edges = (
    filtered_df.groupby(["parent_user", "author"])["score"].sum().reset_index()
)



# Add 1 to the summed scores for the edge weight
user_edges["weight"] = user_edges["score"] + 1

# Create a directed graph
G_weighted = nx.DiGraph()

# Add edges with calculated weights and search_query as an edge attribute
for _, row in tqdm(filtered_df.iterrows(), desc="Building Graph"):
    G_weighted.add_edge(
        row["parent_user"], 
        row["author"], 
        weight=row["score"] + 1, 
        search_query=row["search_query"],
        moon=row["moon"],  # Add 'moon' as an edge attribute
        pump=row["pump"]   # Add 'pump' as an edge attribute
    )
```

```{python}

plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_weighted, seed=42, k=60, scale=3.0)

# Draw nodes and edges
nx.draw_networkx_nodes(G_weighted, pos, node_size=1, alpha=0.7)
nx.draw_networkx_edges(G_weighted, pos, width=1, alpha=0.5, arrowstyle="->", arrowsize=8)

edge_labels = nx.get_edge_attributes(G_weighted, "weight")
for (u, v), label in edge_labels.items():
    x, y = (pos[u][0] + pos[v][0]) / 2, (pos[u][1] + pos[v][1]) / 2  # Midpoint of edge
    plt.text(x, y, f"{label:.1f}", fontsize=6, color="black", alpha=0.9)

plt.title(
    "User Interaction Graph with Weighted Edges (Filtered by Score > 0 and Newest 500 per Search Query)",
    fontsize=16,
)
plt.axis("off")
plt.show()
```

```{python}
G_undirected = G_weighted.to_undirected()

# Perform Label Propagation Clustering on the undirected graph
tqdm_desc = "Label Propagation Clustering"
with tqdm(total=1, desc=tqdm_desc) as pbar:
    communities = list(label_propagation_communities(G_undirected))
    pbar.update(1)

# Assign communities to nodes
node_community_mapping = {node: i for i, community in enumerate(communities) for node in community}
for node, community in node_community_mapping.items():
    G_undirected.nodes[node]['community'] = community

# Display the number of clusters
num_clusters = len(communities)
print(f"Label Propagation Clustering completed: {num_clusters} clusters found.")
```

```{python}
#print size of clusters
cluster_sizes = Counter(node_community_mapping.values())
#sort
cluster_sizes = dict(sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True))
print("Size of clusters:")
print(cluster_sizes)
```

```{python}
self_edges = [(u, v) for u, v in G_undirected.edges() if u == v]
non_self_edges = [(u, v) for u, v in G_undirected.edges() if u != v]

# Create subgraphs for self-references and non-self-references
G_self = G_undirected.edge_subgraph(self_edges).copy()
G_non_self = G_undirected.edge_subgraph(non_self_edges).copy()

# Set up subplots
fig, axes = plt.subplots(1, 2, figsize=(20, 10))
fig.suptitle("Self-references vs Non-self-references in User Interaction Graph (with Clusters)", fontsize=18)

# Plot self-references with clusters
ax = axes[0]
pos_self = nx.spring_layout(G_self, seed=42, k=0.1)
community_colors_self = [G_self.nodes[node]['community'] for node in G_self.nodes()]
nx.draw_networkx_nodes(G_self, pos_self, node_size=50, alpha=0.8, ax=ax, node_color=community_colors_self, cmap=plt.cm.Set3)
nx.draw_networkx_edges(G_self, pos_self, width=1.5, alpha=0.6, ax=ax)
ax.set_title("Self-references (Edges on Themselves)", fontsize=16)
ax.axis("off")

# Plot non-self-references with clusters
ax = axes[1]
pos_non_self = nx.spring_layout(G_non_self, seed=42)
community_colors_non_self = [G_non_self.nodes[node]['community'] for node in G_non_self.nodes()]
nx.draw_networkx_nodes(G_non_self, pos_non_self, node_size=50, alpha=0.8, ax=ax, node_color=community_colors_non_self, cmap=plt.cm.Set3)
nx.draw_networkx_edges(G_non_self, pos_non_self, width=1.5, alpha=0.6, ax=ax)
ax.set_title("Non-self-references (Edges with No Self-references)", fontsize=16)
ax.axis("off")

plt.show()
```

```{python}
cluster_sizes = Counter(nx.get_node_attributes(G_undirected, 'community').values())

# Filter for clusters greater than size 2
valid_clusters = {cluster for cluster, size in cluster_sizes.items() if size > 10}

# Filter nodes belonging to valid clusters
nodes_in_valid_clusters = [node for node in G_undirected.nodes if G_undirected.nodes[node]['community'] in valid_clusters]

# Create a subgraph with only the valid clusters
G_filtered = G_undirected.subgraph(nodes_in_valid_clusters).copy()

# Plot the filtered graph
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_filtered, k=0.1, seed=42)

# Draw nodes and edges
community_colors = [G_filtered.nodes[node]['community'] for node in G_filtered.nodes()]
nx.draw_networkx_nodes(G_filtered, pos, node_size=10, alpha=0.8, node_color=community_colors, cmap=plt.cm.Set3)
nx.draw_networkx_edges(G_filtered, pos, width=1.5, alpha=0.6)

plt.title("Filtered User Interaction Graph (Clusters with Size > 2)", fontsize=16)
plt.axis("off")
plt.show()
```

```{python}
node_search_queries = {node: [] for node in G_undirected.nodes}

# Collect search queries for each node based on connected edges
for u, v, data in G_undirected.edges(data=True):
    if "search_query" in data:
        node_search_queries[u].append(data["search_query"])
        node_search_queries[v].append(data["search_query"])

# Assign the most common 'search_query' to each node
for node, queries in node_search_queries.items():
    most_common_query = Counter(queries).most_common(1)
    G_undirected.nodes[node]["search_query"] = most_common_query[0][0] if most_common_query else "Unknown"

# Identify clusters with size > 10
cluster_sizes = Counter(nx.get_node_attributes(G_undirected, "community").values())
valid_clusters = {cluster for cluster, size in cluster_sizes.items() if size > 10}

# Filter nodes belonging to valid clusters
nodes_in_valid_clusters = [
    node for node in G_undirected.nodes 
    if G_undirected.nodes[node]["community"] in valid_clusters
]

# Create a subgraph with only the valid clusters
G_filtered = G_undirected.subgraph(nodes_in_valid_clusters).copy()

# Assign colors based on 'search_query'
search_query_colors = nx.get_node_attributes(G_filtered, "search_query")
unique_queries = list(set(search_query_colors.values()))
color_map = {query: idx for idx, query in enumerate(unique_queries)}
color_values = [color_map[search_query_colors[node]] for node in G_filtered.nodes()]

# Plot the filtered graph
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_filtered, k=0.1)

nx.draw_networkx_nodes(G_filtered, pos, node_size=10, alpha=0.8, node_color=color_values, cmap=plt.cm.tab20)
nx.draw_networkx_edges(G_filtered, pos, width=1.5, alpha=0.6)

plt.title("Filtered User Interaction Graph (Clusters with Size > 10, Colored by Search Query)", fontsize=16)
plt.axis("off")
plt.show()
```

```{python}
search_queries = set(nx.get_edge_attributes(G_filtered, "search_query").values())

# Create and plot a subgraph for each search query
for search_query in search_queries:
    # Filter edges with the current search query
    edges_for_query = [
        (u, v) for u, v, data in G_filtered.edges(data=True) if data["search_query"] == search_query
    ]

    # Create a subgraph for the current search query
    G_query = G_filtered.edge_subgraph(edges_for_query).copy()

    # Get cluster colors for the nodes
    cluster_colors = [G_query.nodes[node]["community"] for node in G_query.nodes()]
    unique_clusters = list(set(cluster_colors))
    cluster_color_map = {cluster: idx for idx, cluster in enumerate(unique_clusters)}
    node_colors = [cluster_color_map[G_query.nodes[node]["community"]] for node in G_query.nodes()]

    # Plot the graph
    plt.figure(figsize=(12, 10))
    pos = nx.spring_layout(G_query, k=0.1)

    nx.draw_networkx_nodes(G_query, pos, node_size=10, alpha=0.8, node_color=node_colors, cmap=plt.cm.tab20)
    nx.draw_networkx_edges(G_query, pos, width=1.5, alpha=0.6)

    plt.title(f"User Interaction Graph for Search Query: {search_query}", fontsize=16)
    plt.axis("off")
    plt.show()
```

```{python}
search_queries = set(nx.get_edge_attributes(G_filtered, "search_query").values())

# Create and plot a subgraph for each search query with modularity labels
for search_query in search_queries:
    # Filter edges with the current search query
    edges_for_query = [
        (u, v) for u, v, data in G_filtered.edges(data=True) if data["search_query"] == search_query
    ]

    # Create a subgraph for the current search query
    G_query = G_filtered.edge_subgraph(edges_for_query).copy()

    # Identify communities
    communities = {}
    for node in G_query.nodes:
        community = G_query.nodes[node]["community"]
        if community not in communities:
            communities[community] = set()
        communities[community].add(node)

    # Calculate modularity
    modularity_value = modularity(G_query, communities.values())

    # Get cluster colors for the nodes
    cluster_colors = [G_query.nodes[node]["community"] for node in G_query.nodes()]
    unique_clusters = list(set(cluster_colors))
    cluster_color_map = {cluster: idx for idx, cluster in enumerate(unique_clusters)}
    node_colors = [cluster_color_map[G_query.nodes[node]["community"]] for node in G_query.nodes()]

    # Plot the graph
    plt.figure(figsize=(12, 10))
    pos = nx.spring_layout(G_query, k=0.1)

    nx.draw_networkx_nodes(G_query, pos, node_size=10, alpha=0.8, node_color=node_colors, cmap=plt.cm.tab20)
    nx.draw_networkx_edges(G_query, pos, width=1.5, alpha=0.6)

    # Add modularity label
    plt.text(
        0.5, 0.95, f"Modularity: {modularity_value:.3f}", fontsize=14, color="black",
        horizontalalignment='center', transform=plt.gca().transAxes
    )

    plt.title(f"User Interaction Graph for Search Query: {search_query}", fontsize=16)
    plt.axis("off")
    plt.show()
```

```{python}
def calculate_internal_density(cluster_nodes, graph):
    subgraph = graph.subgraph(cluster_nodes)
    num_edges = subgraph.number_of_edges()
    num_nodes = subgraph.number_of_nodes()
    if num_nodes <= 1:
        return 0  # Avoid division by zero
    possible_edges = num_nodes * (num_nodes - 1) / 2
    return num_edges / possible_edges if possible_edges > 0 else 0

# Get unique search queries from the graph
search_queries = set(nx.get_edge_attributes(G_filtered, "search_query").values())

# Create and plot a subgraph for each search query with internal density labels
for search_query in search_queries:
    # Filter edges with the current search query
    edges_for_query = [
        (u, v) for u, v, data in G_filtered.edges(data=True) if data["search_query"] == search_query
    ]

    # Create a subgraph for the current search query
    G_query = G_filtered.edge_subgraph(edges_for_query).copy()

    # Identify communities
    communities = {}
    for node in G_query.nodes:
        community = G_query.nodes[node]["community"]
        if community not in communities:
            communities[community] = set()
        communities[community].add(node)

    # Calculate internal density for each community
    community_densities = {}
    for community, nodes in communities.items():
        community_densities[community] = calculate_internal_density(nodes, G_query)

    # Get cluster colors for the nodes
    cluster_colors = [G_query.nodes[node]["community"] for node in G_query.nodes()]
    unique_clusters = list(set(cluster_colors))
    cluster_color_map = {cluster: idx for idx, cluster in enumerate(unique_clusters)}
    node_colors = [cluster_color_map[G_query.nodes[node]["community"]] for node in G_query.nodes()]

    # Plot the graph
    plt.figure(figsize=(12, 10))
    pos = nx.spring_layout(G_query, k=0.1)

    nx.draw_networkx_nodes(G_query, pos, node_size=10, alpha=0.8, node_color=node_colors, cmap=plt.cm.tab20)
    nx.draw_networkx_edges(G_query, pos, width=1.5, alpha=0.6)

    # Add internal density labels for each cluster
    for community, density in community_densities.items():
        cluster_nodes = communities[community]
        cluster_center = [pos[node] for node in cluster_nodes]
        center_x = sum([coord[0] for coord in cluster_center]) / len(cluster_center)
        center_y = sum([coord[1] for coord in cluster_center]) / len(cluster_center)
        plt.text(
            center_x, center_y, f"{density:.3f}", fontsize=10, color="black", 
            horizontalalignment='center', bbox=dict(facecolor="white", alpha=0.5, boxstyle="round,pad=0.3")
        )

    plt.title(f"User Interaction Graph for Search Query: {search_query} (Internal Density)", fontsize=16)
    plt.axis("off")
    plt.show()
```

```{python}
search_queries = set(nx.get_edge_attributes(G_filtered, "search_query").values())
modularity_values = []

for search_query in search_queries:
    # Filter edges with the current search query
    edges_for_query = [
        (u, v) for u, v, data in G_filtered.edges(data=True) if data["search_query"] == search_query
    ]

    # Create a subgraph for the current search query
    G_query = G_filtered.edge_subgraph(edges_for_query).copy()

    # Identify communities
    communities = {}
    for node in G_query.nodes:
        community = G_query.nodes[node]["community"]
        if community not in communities:
            communities[community] = set()
        communities[community].add(node)

    # Calculate modularity
    modularity_value = modularity(G_query, communities.values())
    modularity_values.append((search_query, modularity_value))

# Sort the modularity values for better visualization
modularity_values.sort(key=lambda x: x[1], reverse=True)

# Extract data for plotting
queries = [item[0] for item in modularity_values]
modularity_scores = [item[1] for item in modularity_values]

# Create the barplot
plt.figure(figsize=(12, 8))
plt.barh(queries, modularity_scores, color='skyblue', edgecolor='black')
plt.xlabel("Modularity", fontsize=14)
plt.ylabel("Search Query", fontsize=14)
plt.title("Modularity Scores for Each Search Query", fontsize=16)
plt.gca().invert_yaxis()  # Invert y-axis for better readability
plt.grid(axis='x', linestyle='--', alpha=0.7)
plt.show()
```


# Introduction
## Objective
### State the overarching goal of the project
### Define specific questions or hypotheses to be explored through SNA

## Scope
### Define the boundaries of the analysis (e.g., time frame, data granularity)

## Relevance to SNA
### Discuss why this topic is suitable for SNA
### Highlight potential insights or applications of the findings

## Data Sources
### List sources of data and their formats (e.g., API, CSV, JSON)
### Mention any limitations or conditions tied to the data


# Data Collection
## Objective
### Summarize the goal of collecting this data

## Selection Criteria
### Describe the factors considered while selecting datasets (e.g., attributes, volume, quality)

## Methods Used
### API usage or web scraping
#### Describe API endpoints or scraping scripts
#### Tools used (e.g., `requests`, `BeautifulSoup`)
### Pre-existing datasets
#### Mention the sources and any preprocessing required

## Challenges and Solutions
### Examples of issues encountered (e.g., data access restrictions, API rate limits)
### Workarounds or adjustments made

## Data Validation
### Describe checks performed to ensure completeness and accuracy
### Strategies for handling missing or inconsistent data

## Raw Data Summary
### Present a summary table (e.g., number of records, key fields)
### Provide a brief description of important fields

# Exploratory Data Analysis (EDA)
## Initial Insights
### Describe first impressions of the dataset

## Data Cleaning
### Steps taken to clean data (e.g., handling duplicates, standardizing formats)

## Visual Exploration
### Histograms, scatter plots, and other visualizations for understanding distributions

## Statistical Analysis
### Summarize key statistics (mean, median, mode, variance)

## Missing Data Analysis
### Identify missing values and their impact
### Strategies for imputation or exclusion

## Outliers
### Discuss detection methods (e.g., box plots, z-scores)
### How these were handled in the dataset

## Tools and Automation
### Summary of libraries/tools used for EDA (e.g., `pandas-profiling`, `Sweetviz`)
### Include generated reports if applicable

# Data Modeling
## Network Design
### Define the network type (e.g., directed, undirected, weighted)
### Specify attributes for nodes and edges

## Preprocessing
### Steps for transforming raw data into a network-ready format

## Graph Construction
### Description of how the graph was built (e.g., adjacency matrix, edge list)

## Graph Characteristics
### Number of nodes and edges
### Basic statistics (density, clustering coefficient)

## Visualization
### Initial network plots (e.g., using `NetworkX`, `Matplotlib`, Gephi)
### Brief interpretation of the visualizations

# Social Network Analysis
## Overview of Analyses
### List the types of analyses to be performed

## Node-Level Metrics
### Calculate and interpret Degree Centrality
### Calculate and interpret Closeness Centrality
### Calculate and interpret Betweenness Centrality
### Calculate and interpret Eigenvector Centrality

## Edge-Level Analysis
### Identify and interpret important connections

## Community Detection
### Discuss methods used (e.g., Louvain, Girvan-Newman)
### Visualize and interpret communities

## Temporal or Dynamic Analysis
### If applicable, analyze changes in the network over time

## Sub-Networks
### Identify key sub-networks
### Discuss their relevance and characteristics

## Comparisons
### Compare multiple networks or perspectives within the same dataset

## Visualization and Interpretation
### Highlight key findings with clear visual aids
### Relate findings to the original research question

# Results and Interpretation
## Key Findings
### Summarize the main results from the analyses

## Insights
### Discuss how these results address the research questions or hypotheses

## Real-World Implications
### Explain the significance of these findings in a broader context

## Evaluation
### Assess the quality of the data and analyses

## Limitations
### Acknowledge shortcomings of the data or methods

# Outlook
## Future Work
### Suggest extensions or improvements to the project

## Unanswered Questions
### Highlight potential areas of exploration

## Data Enhancements
### Discuss additional data sources or attributes that could enrich the analysis

# Lessons Learned
## Project Highlights
### Reflect on successes and innovative solutions

## Challenges
### Discuss difficulties faced and lessons drawn

## Recommendations
### Propose best practices for similar projects in the future

# Appendices
## Code and Scripts
### Include all relevant scripts used for the project

## Raw Data Samples
### Attach examples of the raw dataset

## Graph Outputs
### Save and display network outputs (e.g., CSVs, Gephi files)

## Additional Visualizations
### Supplementary charts or tables

## References
### Cite all external sources and tools used


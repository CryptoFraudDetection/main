---
title: Setup
jupyter: python3
---



```{python}
import numpy as np
import altair as alt
import pandas as pd
import matplotlib.pyplot as plt

alt.themes.enable("dark")
```

Read data from parquet file:

```{python}
df = pd.read_parquet("../data/processed/reddit.parquet")
```

## EDA

Glimpse of the data:

```{python}
df.sample(5, random_state=42)
```

Which coin appears how often in the data?

```{python}
df["search_query"].value_counts()
```

Overview of comments (answers to other people) per coin and subreddit:

```{python}
subreddit_query = (
    df.groupby(["search_query", "subreddit"])
    .size()
    .unstack()
    .fillna(0)
    .astype(int)
    .reset_index()
    .melt(id_vars="search_query")
)

c = (
    alt.Chart(subreddit_query)
    .mark_bar()
    .encode(
        x=alt.X("value:Q", title="Number of Comments", scale={"domain": [0, 152_000]}),
        y=alt.Y("search_query:O", title="Coin"),
        color=alt.Color("subreddit:N", title="Subreddit"),
        tooltip=[
            alt.Tooltip("search_query:O", title="Coin"),
            alt.Tooltip("subreddit:N", title="Subreddit"),
            alt.Tooltip("value:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Comments per Subreddit"),
    )
)


text = (
    alt.Chart(subreddit_query)
    .mark_text(align="left", dx=5, color="white")
    .transform_calculate(customtooltip="datum.value")
    .encode(
        x=alt.X("sum(value):Q", scale={"domain": [0, 152_000]}),
        y=alt.Y("search_query:O"),
        text=alt.Text("sum(value):Q"),
    )
)

c + text
```

It's clearly visible that Bitcoin is the most popular coin and CryptoCurrency is the most popular subreddit. The interest is, how fraudulent and non-fraudulent coins differ in there social media activity. Some coins (like Teddy Doge and BeerCoin) have only a few comments.

Overview of main posts per coin and subreddit:

```{python}
subreddit_query_posts = (
    df.query("depth == -1")
    .groupby(["search_query", "subreddit"])
    .size()
    .unstack()
    .fillna(0)
    .astype(int)
    .reset_index()
    .melt(id_vars="search_query")
)

c = (
    alt.Chart(subreddit_query_posts)
    .mark_bar()
    .encode(
        x=alt.X("value:Q", title="Number of Comments", scale={"domain": [0, 1_400]}),
        y=alt.Y("search_query:O", title="Coin"),
        color=alt.Color("subreddit:N", title="Subreddit"),
        tooltip=[
            alt.Tooltip("search_query:O", title="Coin"),
            alt.Tooltip("subreddit:N", title="Subreddit"),
            alt.Tooltip("value:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Posts per Subreddit"),
    )
)


text = (
    alt.Chart(subreddit_query_posts)
    .mark_text(align="left", dx=5, color="white")
    .transform_calculate(customtooltip="datum.value")
    .encode(
        x=alt.X("sum(value):Q", scale={"domain": [0, 1_400]}),
        y=alt.Y("search_query:O"),
        text=alt.Text("sum(value):Q"),
    )
)

c + text
```

The distribution of posts is similar to the distribution of comments. Teddy Doge got only a few posts. This should not be an issue, as the comments (replies to other people) will be the edges in the graph.

How deeply nested are the comments (replies to other people)?

```{python}
depth_df = df["depth"].value_counts().sort_index()

c = (
    alt.Chart(depth_df.reset_index())
    .mark_bar()
    .encode(
        x=alt.X("depth:O", title="Depth (-1 is the original post)"),
        y=alt.Y("count:Q", title="Number of Comments", scale={"domain": [0, 240_000]}),
        tooltip=[
            alt.Tooltip("depth:O", title="Depth"),
            alt.Tooltip("count:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Comments per Depth"),
    )
)

text = (
    alt.Chart(depth_df.reset_index())
    .mark_text(align="center", dy=-8, color="white")
    .transform_calculate(customtooltip="datum.count")
    .encode(
        x=alt.X("depth:O"),
        y=alt.Y("count:Q", scale={"domain": [0, 240_000]}),
        text=alt.Text("count:Q"),
    )
)

c + text
```

There are ~6k posts and most comments (~200k) are direct replies to the post. The distribution decreases ~exponentially and goes to 10 levels deep.

How are the number of comments distributed per user?

```{python}
comments_per_user = df.groupby("author").size()
distribution = comments_per_user.value_counts().sort_index()
distribution_df = distribution.reset_index()
distribution_df.columns = ["comments_per_user", "number_of_users"]

c = (
    alt.Chart(distribution_df)
    .mark_bar()
    .encode(
        x=alt.X(
            "comments_per_user:O",
            title="Number of Comments per User (cut off at 100)",
            scale={"domain": range(1, 101)},
        ),
        y=alt.Y(
            "number_of_users:Q",
            title="Number of Users",
            scale=alt.Scale(
                type="symlog",
                domain=[0, 100_000],
            ),
            axis=alt.Axis(
                values=[
                    0,
                    1,
                    2,
                    5,
                    10,
                    20,
                    50,
                    100,
                    200,
                    500,
                    1_000,
                    2_000,
                    5_000,
                    10_000,
                    20_000,
                    50_000,
                    100_000,
                ]
            ),
        ),
        tooltip=[
            alt.Tooltip("comments_per_user:Q", title="# of Comments per User"),
            alt.Tooltip("number_of_users:Q", title="# of Users"),
        ],
    )
    .properties(
        width=1000,
        height=400,
        title=alt.Title(text="Number of Comments per User"),
    )
)

c
```

Most users have only a few comments (~70k have only one comment). There are some users with a lot of comments (cut off at 100). How extreme are the top users?

```{python}
comments_per_user.sort_values(ascending=False).head(10)
```

Some users have a lot of comments but they have `bot`, `auto` or `mod` in their name, which means that they are probably no human. They might be official bots from the subreddit for moderation purposes.

Now lets see how the posts are distributed over the users:

```{python}
posts_per_user = df.query("depth == -1").groupby("author").size()
distribution = posts_per_user.value_counts().sort_index()
distribution_df = distribution.reset_index()
distribution_df.columns = ["posts_per_user", "number_of_users"]

c = (
    alt.Chart(distribution_df)
    .mark_bar()
    .encode(
        x=alt.X(
            "posts_per_user:O",
            title="Number of Posts per User (cut off at 100)",
            scale={"domain": range(1, 101)},
        ),
        y=alt.Y(
            "number_of_users:Q",
            title="Number of Users",
            scale=alt.Scale(
                type="symlog",
                domain=[0, 5_000],
                zero=True,
            ),
            axis=alt.Axis(
                values=[
                    0,
                    1,
                    2,
                    5,
                    10,
                    20,
                    50,
                    100,
                    200,
                    500,
                    1_000,
                    2_000,
                    5_000,
                ]
            ),
        ),
        tooltip=[
            alt.Tooltip("posts_per_user:Q", title="# of Posts per User"),
            alt.Tooltip("number_of_users:Q", title="# of Users"),
        ],
    )
    .properties(
        width=1000,
        height=400,
        title=alt.Title(text="Number of Posts per User"),
    )
)

c
```

The posts are distributed unevenly as well. ~3.6k users have only one post. Let's have a look at the top users (for posts):

```{python}
posts_per_user.sort_values(ascending=False).head(10)
```

Some users overlap with the top commenters. Examples: kirtash93, goldyluckinblokchain

Let's have a look at the comment distribution over time per coin:

```{python}
posts_per_coin_per_date = df.copy()
posts_per_coin_per_date["created"] = (
    pd.to_datetime(posts_per_coin_per_date["created"])
    .dt.to_period("M")
    .dt.to_timestamp()
)

# Filter for comments with depth 0 to 9
posts_per_coin_per_date = posts_per_coin_per_date.query(
    "created >= '2020-01-01'"
).query("depth >= 0 and depth <= 9")

posts_per_coin_per_date = (
    posts_per_coin_per_date.groupby(["search_query", "created"])["id"]
    .count()
    .reset_index()
    .rename(columns={"id": "number_of_comments"})
)

posts_per_coin_per_date["number_of_comments_rel"] = posts_per_coin_per_date[
    "number_of_comments"
] / posts_per_coin_per_date.groupby("search_query")["number_of_comments"].transform("sum")

step = 40
overlap = 1

c = (
    alt.Chart(posts_per_coin_per_date, height=step)
    .mark_area(
        interpolate="monotone",
        fillOpacity=0.5,
        stroke="lightgray",
        strokeWidth=0.5,
    )
    .encode(
        x=alt.X("created:T").title("Date").axis(grid=False),
        y=alt.Y("number_of_comments_rel:Q")
        .axis(None)
        .scale(range=[step, -step * overlap]),
        color=alt.Color("search_query:N", legend=None),
        tooltip=[
            alt.Tooltip("yearmonth(created):T", title="Date"),
            alt.Tooltip("search_query:N", title="Coin"),
            alt.Tooltip("number_of_comments:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=step,
    )
    .facet(
        row=alt.Row("search_query:N")
        .title(None)
        .header(labelAngle=0, labelAlign="left")
    )
    .properties(
        title=alt.Title(
            text="Number of Comments per Coin per Date after 2020",
            anchor="middle",
        ),
        bounds="flush",
    )
    .configure_facet(spacing=0)
    .configure_view(stroke=None)
    .configure_title(anchor="end")
)

c
```

Let's have a look at the comment distribution over time per coin:

```{python}
posts_per_coin_per_date = df.copy()
posts_per_coin_per_date["created"] = (
    pd.to_datetime(posts_per_coin_per_date["created"])
    .dt.to_period("M")
    .dt.to_timestamp()
)

posts_per_coin_per_date = posts_per_coin_per_date.query(
    "created >= '2020-01-01'"
).query("depth == -1")


posts_per_coin_per_date = (
    posts_per_coin_per_date.groupby(["search_query", "created"])["id"]
    .count()
    .reset_index()
    .rename(columns={"id": "number_of_posts"})
)

posts_per_coin_per_date["number_of_posts_rel"] = posts_per_coin_per_date[
    "number_of_posts"
] / posts_per_coin_per_date.groupby("search_query")["number_of_posts"].transform("sum")

posts_per_coin_per_date

step = 40
overlap = 1

c = (
    alt.Chart(posts_per_coin_per_date, height=step)
    .mark_area(
        interpolate="monotone",
        fillOpacity=0.5,
        stroke="lightgray",
        strokeWidth=0.5,
    )
    .encode(
        x=alt.X("created:T").title("Date").axis(grid=False),
        y=alt.Y("number_of_posts_rel:Q")
        .axis(None)
        .scale(range=[step, -step * overlap]),
        color=alt.Color("search_query:N", legend=None),
        tooltip=[
            alt.Tooltip("yearmonth(created):T", title="Date"),
            alt.Tooltip("search_query:N", title="Coin"),
            alt.Tooltip("number_of_posts:Q", title="# of Posts"),
        ],
    )
    .properties(
        width=800,
        height=step,
    )
    .facet(
        row=alt.Row("search_query:N")
        .title(None)
        .header(labelAngle=0, labelAlign="left")
    )
    .properties(
        title=alt.Title(
            text="Number of Posts per Coin per Date after 2020",
            anchor="middle",
        ),
        bounds="flush",
    )
    .configure_facet(spacing=0)
    .configure_view(stroke=None)
    .configure_title(anchor="end")
)

c
```

The plot is cut off at 2020 because there are not many posts or comments before that time (in relation to the rest of the data). Some cions have more data at the end of the timescale because we used google results to get the post URLs and google had probably some bais towards newer posts.

# Introduction
## Objective
### State the overarching goal of the project
### Define specific questions or hypotheses to be explored through SNA

## Scope
### Define the boundaries of the analysis (e.g., time frame, data granularity)

## Relevance to SNA
### Discuss why this topic is suitable for SNA
### Highlight potential insights or applications of the findings

## Data Sources
### List sources of data and their formats (e.g., API, CSV, JSON)
### Mention any limitations or conditions tied to the data


# Data Collection
## Objective
### Summarize the goal of collecting this data

## Selection Criteria
### Describe the factors considered while selecting datasets (e.g., attributes, volume, quality)

## Methods Used
### API usage or web scraping
#### Describe API endpoints or scraping scripts
#### Tools used (e.g., `requests`, `BeautifulSoup`)
### Pre-existing datasets
#### Mention the sources and any preprocessing required

## Challenges and Solutions
### Examples of issues encountered (e.g., data access restrictions, API rate limits)
### Workarounds or adjustments made

## Data Validation
### Describe checks performed to ensure completeness and accuracy
### Strategies for handling missing or inconsistent data

## Raw Data Summary
### Present a summary table (e.g., number of records, key fields)
### Provide a brief description of important fields

# Exploratory Data Analysis (EDA)
## Initial Insights
### Describe first impressions of the dataset

## Data Cleaning
### Steps taken to clean data (e.g., handling duplicates, standardizing formats)

## Visual Exploration
### Histograms, scatter plots, and other visualizations for understanding distributions

## Statistical Analysis
### Summarize key statistics (mean, median, mode, variance)

## Missing Data Analysis
### Identify missing values and their impact
### Strategies for imputation or exclusion

## Outliers
### Discuss detection methods (e.g., box plots, z-scores)
### How these were handled in the dataset

## Tools and Automation
### Summary of libraries/tools used for EDA (e.g., `pandas-profiling`, `Sweetviz`)
### Include generated reports if applicable

# Data Modeling
## Network Design
### Define the network type (e.g., directed, undirected, weighted)
### Specify attributes for nodes and edges

## Preprocessing
### Steps for transforming raw data into a network-ready format

## Graph Construction
### Description of how the graph was built (e.g., adjacency matrix, edge list)

## Graph Characteristics
### Number of nodes and edges
### Basic statistics (density, clustering coefficient)

## Visualization
### Initial network plots (e.g., using `NetworkX`, `Matplotlib`, Gephi)
### Brief interpretation of the visualizations

# Social Network Analysis
## Overview of Analyses
### List the types of analyses to be performed

## Node-Level Metrics
### Calculate and interpret Degree Centrality
### Calculate and interpret Closeness Centrality
### Calculate and interpret Betweenness Centrality
### Calculate and interpret Eigenvector Centrality

## Edge-Level Analysis
### Identify and interpret important connections

## Community Detection
### Discuss methods used (e.g., Louvain, Girvan-Newman)
### Visualize and interpret communities

## Temporal or Dynamic Analysis
### If applicable, analyze changes in the network over time

## Sub-Networks
### Identify key sub-networks
### Discuss their relevance and characteristics

## Comparisons
### Compare multiple networks or perspectives within the same dataset

## Visualization and Interpretation
### Highlight key findings with clear visual aids
### Relate findings to the original research question

# Results and Interpretation
## Key Findings
### Summarize the main results from the analyses

## Insights
### Discuss how these results address the research questions or hypotheses

## Real-World Implications
### Explain the significance of these findings in a broader context

## Evaluation
### Assess the quality of the data and analyses

## Limitations
### Acknowledge shortcomings of the data or methods

# Outlook
## Future Work
### Suggest extensions or improvements to the project

## Unanswered Questions
### Highlight potential areas of exploration

## Data Enhancements
### Discuss additional data sources or attributes that could enrich the analysis

# Lessons Learned
## Project Highlights
### Reflect on successes and innovative solutions

## Challenges
### Discuss difficulties faced and lessons drawn

## Recommendations
### Propose best practices for similar projects in the future

# Appendices
## Code and Scripts
### Include all relevant scripts used for the project

## Raw Data Samples
### Attach examples of the raw dataset

## Graph Outputs
### Save and display network outputs (e.g., CSVs, Gephi files)

## Additional Visualizations
### Supplementary charts or tables

## References
### Cite all external sources and tools used


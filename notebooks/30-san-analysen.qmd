---
title: Setup
jupyter: python3
---

```{python}
from typing import Any
import numpy as np
import altair as alt
import pandas as pd
import matplotlib.pyplot as plt
import networkx as nx
import os
import random
import re
from tqdm.notebook import tqdm
from collections import Counter
from community import community_louvain
from networkx.algorithms.community import label_propagation_communities
from networkx.algorithms.community.quality import modularity
import heapq
from IPython.display import Image, display
import pygraphviz as pgv

alt.themes.enable("dark")
```

Read data from parquet file:

```{python}
df = pd.read_parquet("../data/processed/reddit.parquet")
datetime_columns = ["edited", "created"]
df[datetime_columns] = df[datetime_columns].apply(pd.to_datetime, errors="coerce")
```

## EDA

### Data Overview

Glimpse of the data:

```{python}
df.sample(5, random_state=42)
```

Some rows are comments and others are posts: Posts `depth` is set to -1 and they are missing `parent_id` as they initiate a potential discussion. Comments have `parent_id` set to the `id` of the post they are replying to and they are missing `url`, `num_comments`, `title` as they are not posts. Comments can be replies to posts or other comments and therefore a nested structure can be formed.

```{python}
df.dtypes
```

These are the columns in the data. Our Network will be an OneMode Network with `author` nodes and the directed edges being the comments (`depth>=0`).

#### Crypto Coins Frequency

How many posts are there for each coin?

```{python}
df.query("depth == -1")["search_query"].value_counts()
```

Some coins like Teddy Doge and BeerCoin have only a few posts. How many comments are there for each coin?

```{python}
df.query("depth > -1")["search_query"].value_counts()
```

There are multiple hundreds of commeents for each coin or more. As the comments are more important to this analysis and the coins are therefore kept.

#### Score Distribution

How are the scores distributed over the posts and comments in respect to there depth (reply to reply to reply etc.)? Note that posts have `depth` set to `-1`.

```{python}
depths = range(-1, 10)  # Tiefen von -1 bis 9
scores_by_depth = [df[df["depth"] == depth]["score"].dropna() for depth in depths]

means_by_depth = [scores.mean() for scores in scores_by_depth]

# Erstelle die Boxplots
plt.figure(figsize=(12, 8))
plt.boxplot(
    scores_by_depth,
    vert=False,
    patch_artist=True,
    tick_labels=[f"Depth {depth}" for depth in depths],
    boxprops=dict(facecolor="lightblue", color="blue"),
    medianprops=dict(color="red", linewidth=1.5),
    flierprops=dict(marker="o", color="orange", markersize=5, alpha=0.5),
)

# Zeichne vertikale Linien f√ºr die Mittelwerte ein
for i, mean in enumerate(means_by_depth):
    plt.axvline(
        x=mean,
        ymin=(i + 0.25) / len(depths),
        ymax=(i + 0.75) / len(depths),
        color="green",
        linewidth=1.5,
        label="Mean" if i == 0 else "",
    )

# Plot-Details
plt.title("Boxplot of Scores by Depth (with Mean)", fontsize=14)
plt.xlabel("Score (Log scaled)", fontsize=12)
plt.ylabel("Depth", fontsize=12)
plt.xscale("log")
plt.grid(axis="x", linestyle="--", alpha=0.7)
plt.legend()
plt.show()
```

In this plot its clearly visible, that deeper nested comments usually have a lower score. Posts have extreme outliers, which is visible in the logaritmic X-axis. The Distribution of the score is right-skewed because the mean is higher than the median.

#### Comments and Posts Distributions per Coin and Subreddit

Overview of comments (answers to other people) per coin and subreddit:

```{python}
subreddit_query = (
    df.groupby(["search_query", "subreddit"])
    .size()
    .unstack()
    .fillna(0)
    .astype(int)
    .reset_index()
    .melt(id_vars="search_query")
)

c = (
    alt.Chart(subreddit_query)
    .mark_bar()
    .encode(
        x=alt.X("value:Q", title="Number of Comments", scale={"domain": [0, 152_000]}),
        y=alt.Y("search_query:O", title="Coin"),
        color=alt.Color("subreddit:N", title="Subreddit"),
        tooltip=[
            alt.Tooltip("search_query:O", title="Coin"),
            alt.Tooltip("subreddit:N", title="Subreddit"),
            alt.Tooltip("value:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Comments per Subreddit"),
    )
)


text = (
    alt.Chart(subreddit_query)
    .mark_text(align="left", dx=5, color="white")
    .transform_calculate(customtooltip="datum.value")
    .encode(
        x=alt.X("sum(value):Q", scale={"domain": [0, 152_000]}),
        y=alt.Y("search_query:O"),
        text=alt.Text("sum(value):Q"),
    )
)

c + text
```

It's clearly visible that Bitcoin is the most popular coin and CryptoCurrency is the most popular subreddit. The interest is, how fraudulent and non-fraudulent coins differ in there social media activity. Some coins (like Teddy Doge and BeerCoin) have only a few comments.

Overview of main posts per coin and subreddit:

```{python}
subreddit_query_posts = (
    df.query("depth == -1")
    .groupby(["search_query", "subreddit"])
    .size()
    .unstack()
    .fillna(0)
    .astype(int)
    .reset_index()
    .melt(id_vars="search_query")
)

c = (
    alt.Chart(subreddit_query_posts)
    .mark_bar()
    .encode(
        x=alt.X("value:Q", title="Number of Comments", scale={"domain": [0, 1_400]}),
        y=alt.Y("search_query:O", title="Coin"),
        color=alt.Color("subreddit:N", title="Subreddit"),
        tooltip=[
            alt.Tooltip("search_query:O", title="Coin"),
            alt.Tooltip("subreddit:N", title="Subreddit"),
            alt.Tooltip("value:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Posts per Subreddit"),
    )
)


text = (
    alt.Chart(subreddit_query_posts)
    .mark_text(align="left", dx=5, color="white")
    .transform_calculate(customtooltip="datum.value")
    .encode(
        x=alt.X("sum(value):Q", scale={"domain": [0, 1_400]}),
        y=alt.Y("search_query:O"),
        text=alt.Text("sum(value):Q"),
    )
)

c + text
```

The distribution of posts is similar to the distribution of comments. Teddy Doge got only a few posts. This should not be an issue, as the comments (replies to other people) will be the edges in the graph.

#### Comments Depth Distribution

How deeply nested are the comments (replies to other people)?

```{python}
depth_df = df["depth"].value_counts().sort_index()

c = (
    alt.Chart(depth_df.reset_index())
    .mark_bar()
    .encode(
        x=alt.X(
            "depth:O",
            title="Depth (-1 is the original post and 0 are top-level comments)",
        ),
        y=alt.Y("count:Q", title="Number of Comments", scale={"domain": [0, 240_000]}),
        tooltip=[
            alt.Tooltip("depth:O", title="Depth"),
            alt.Tooltip("count:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=400,
        title=alt.Title(text="Number of Comments per Depth"),
    )
)

text = (
    alt.Chart(depth_df.reset_index())
    .mark_text(align="center", dy=-8, color="white")
    .transform_calculate(customtooltip="datum.count")
    .encode(
        x=alt.X("depth:O"),
        y=alt.Y("count:Q", scale={"domain": [0, 240_000]}),
        text=alt.Text("count:Q"),
    )
)

c + text
```

There are ~6k posts and most comments (~200k) are direct replies to the post. The distribution decreases ~exponentially and goes to 10 levels deep.

#### Comment/Post Count Distributions per User

How are the number of comments distributed per user?

```{python}
comments_per_user = df.groupby("author").size()
distribution = comments_per_user.value_counts().sort_index()
distribution_df = distribution.reset_index()
distribution_df.columns = ["comments_per_user", "number_of_users"]

c = (
    alt.Chart(distribution_df)
    .mark_bar()
    .encode(
        x=alt.X(
            "comments_per_user:O",
            title="Number of Comments per User (cut off at 100)",
            scale={"domain": range(1, 101)},
        ),
        y=alt.Y(
            "number_of_users:Q",
            title="Number of Users",
            scale=alt.Scale(
                type="symlog",
                domain=[0, 100_000],
            ),
            axis=alt.Axis(
                values=[
                    0,
                    1,
                    2,
                    5,
                    10,
                    20,
                    50,
                    100,
                    200,
                    500,
                    1_000,
                    2_000,
                    5_000,
                    10_000,
                    20_000,
                    50_000,
                    100_000,
                ]
            ),
        ),
        tooltip=[
            alt.Tooltip("comments_per_user:Q", title="# of Comments per User"),
            alt.Tooltip("number_of_users:Q", title="# of Users"),
        ],
    )
    .properties(
        width=1000,
        height=400,
        title=alt.Title(text="Number of Comments per User"),
    )
)

c
```

Most users have only a few comments (~70k have only one comment). There are some users with a lot of comments (cut off at 100). How extreme are the top users?

```{python}
comments_per_user.sort_values(ascending=False).head(10)
```

Some users have a lot of comments but they have `bot`, `auto` or `mod` in their name, they are therefore probably no human. They might be official bots from the subreddit for moderation purposes.

How the posts are distributed over the users:

```{python}
posts_per_user = df.query("depth == -1").groupby("author").size()
distribution = posts_per_user.value_counts().sort_index()
distribution_df = distribution.reset_index()
distribution_df.columns = ["posts_per_user", "number_of_users"]

c = (
    alt.Chart(distribution_df)
    .mark_bar()
    .encode(
        x=alt.X(
            "posts_per_user:O",
            title="Number of Posts per User (cut off at 100)",
            scale={"domain": range(1, 101)},
        ),
        y=alt.Y(
            "number_of_users:Q",
            title="Number of Users",
            scale=alt.Scale(
                type="symlog",
                domain=[0, 5_000],
                zero=True,
            ),
            axis=alt.Axis(
                values=[
                    0,
                    1,
                    2,
                    5,
                    10,
                    20,
                    50,
                    100,
                    200,
                    500,
                    1_000,
                    2_000,
                    5_000,
                ]
            ),
        ),
        tooltip=[
            alt.Tooltip("posts_per_user:Q", title="# of Posts per User"),
            alt.Tooltip("number_of_users:Q", title="# of Users"),
        ],
    )
    .properties(
        width=1000,
        height=400,
        title=alt.Title(text="Number of Posts per User"),
    )
)

c
```

The posts are distributed unevenly as well. ~3.6k users have only one post. How extreme are the top users?

```{python}
posts_per_user.sort_values(ascending=False).head(10)
```

Some users overlap with the top commenters. Examples: `kirtash93`, `goldyluckinblokchain`

#### Comment/Post Distributions over Time

How are the comments and posts distributed over time?

```{python}
posts_per_coin_per_date = df.copy()
posts_per_coin_per_date["created"] = (
    pd.to_datetime(posts_per_coin_per_date["created"])
    .dt.to_period("M")
    .dt.to_timestamp()
)

posts_per_coin_per_date = posts_per_coin_per_date.query(
    "created >= '2020-01-01'"
).query("depth >= 0")

posts_per_coin_per_date = (
    posts_per_coin_per_date.groupby(["search_query", "created"])["id"]
    .count()
    .reset_index()
    .rename(columns={"id": "number_of_comments"})
)

posts_per_coin_per_date["number_of_comments_rel"] = posts_per_coin_per_date[
    "number_of_comments"
] / posts_per_coin_per_date.groupby("search_query")["number_of_comments"].transform(
    "sum"
)

step = 40
overlap = 1

c = (
    alt.Chart(posts_per_coin_per_date, height=step)
    .mark_area(
        interpolate="monotone",
        fillOpacity=0.5,
        stroke="lightgray",
        strokeWidth=0.5,
    )
    .encode(
        x=alt.X("created:T").title("Date").axis(grid=False),
        y=alt.Y("number_of_comments_rel:Q")
        .axis(None)
        .scale(range=[step, -step * overlap]),
        color=alt.Color("search_query:N", legend=None),
        tooltip=[
            alt.Tooltip("yearmonth(created):T", title="Date"),
            alt.Tooltip("search_query:N", title="Coin"),
            alt.Tooltip("number_of_comments:Q", title="# of Comments"),
        ],
    )
    .properties(
        width=800,
        height=step,
    )
    .facet(
        row=alt.Row("search_query:N")
        .title(None)
        .header(labelAngle=0, labelAlign="left")
    )
    .properties(
        title=alt.Title(
            text="Number of Comments per Coin per Date after 2020",
            anchor="middle",
        ),
        bounds="flush",
    )
    .configure_facet(spacing=0)
    .configure_view(stroke=None)
    .configure_title(anchor="end")
)

c
```

```{python}
posts_per_coin_per_date = df.copy()
posts_per_coin_per_date["created"] = (
    pd.to_datetime(posts_per_coin_per_date["created"])
    .dt.to_period("M")
    .dt.to_timestamp()
)

posts_per_coin_per_date = posts_per_coin_per_date.query(
    "created >= '2020-01-01'"
).query("depth == -1")


posts_per_coin_per_date = (
    posts_per_coin_per_date.groupby(["search_query", "created"])["id"]
    .count()
    .reset_index()
    .rename(columns={"id": "number_of_posts"})
)

posts_per_coin_per_date["number_of_posts_rel"] = posts_per_coin_per_date[
    "number_of_posts"
] / posts_per_coin_per_date.groupby("search_query")["number_of_posts"].transform("sum")

posts_per_coin_per_date

step = 40
overlap = 1

c = (
    alt.Chart(posts_per_coin_per_date, height=step)
    .mark_area(
        interpolate="monotone",
        fillOpacity=0.5,
        stroke="lightgray",
        strokeWidth=0.5,
    )
    .encode(
        x=alt.X("created:T").title("Date").axis(grid=False),
        y=alt.Y("number_of_posts_rel:Q")
        .axis(None)
        .scale(range=[step, -step * overlap]),
        color=alt.Color("search_query:N", legend=None),
        tooltip=[
            alt.Tooltip("yearmonth(created):T", title="Date"),
            alt.Tooltip("search_query:N", title="Coin"),
            alt.Tooltip("number_of_posts:Q", title="# of Posts"),
        ],
    )
    .properties(
        width=800,
        height=step,
    )
    .facet(
        row=alt.Row("search_query:N")
        .title(None)
        .header(labelAngle=0, labelAlign="left")
    )
    .properties(
        title=alt.Title(
            text="Number of Posts per Coin per Date after 2020",
            anchor="middle",
        ),
        bounds="flush",
    )
    .configure_facet(spacing=0)
    .configure_view(stroke=None)
    .configure_title(anchor="end")
)

c
```

The plots are cut off at 2020 because there are not many posts or comments before that time (in relation to the rest of the data). Some coins have more data at the end of the timescale because we used google results to get the post URLs and google had probably some bais towards newer posts.

### NA Values

#### Posts

How many posts are there?

```{python}
print("Number of posts:", len(df.query("depth == -1")))
```

`parent_id` is expeted to be NA for all posts. Columns with missing values for posts:

```{python}
df.query("depth == -1").isna().sum()[lambda x: x > 0]
```

Some posts have been edited but this is ignored in this analysis.

#### Comments

How many comments are there?

```{python}
print("Number of comments:", len(df[df["depth"] >= 0]))
```

`title`, `url` and `num_comments` are not used for comments and are expected to be NA. Which columns do have missing values for comments?

```{python}
df.query("depth >= 0").isna().sum()[lambda x: x > 0]
```

`edited` is ignored for comments as well.

Posts are not of primary interest for this analysis and therefore unnecessary columns are dropped (even though they are not always NA for posts):

```{python}
columns = ["edited", "title", "url", "num_comments"]
shrinked_df = df.drop(columns=columns)
print("Dropped columns:")
print(columns)
print("Columns remaining:")
print(shrinked_df.columns.to_list())
```

## User Interaction Network

In this analysis the interactions between users is analyzed and therefore a column `paprent_user` is created with the `author` of the post/comment the user is replying to. This enables the creation of a graph with users as nodes and directed edges being the comments.

### Data Preprocessing

The data is grouped by `author` and `parent_user` so there is only one edge between the same nodes. The edge weights are the number of comments between the two authors. Edges get `sum(score)`, sum of critical word hit counts (example: `moon`) and number of coin occurrences as additional edge attributes.

As some algorithms do not work on directed graphs it might be necessary to convert the graph to an undirected graph later on. The information in which direction the comments went would be lost, but the overall information, which users tend to interact and the word counts will be preserved.

The aggregation is done on the data frame and a glimpse at the data is provided (sorted by some edge attribute):

```{python}
# Convert coin column to dummy columns

dummies_df = shrinked_df.copy()
# convert coin names to snake_case
dummies_df["search_query"] = dummies_df["search_query"].str.replace(r"[^a-zA-Z0-9]", "_", regex=True).str.lower()
# Convert coin column to dummy columns
dummies_df = (
    dummies_df
    .join(dummies_df.search_query.str.get_dummies())
    .drop(columns="search_query")
)

# Add parent user column (author of the comment/post the user is replying to)
parent_user_df = dummies_df.merge(
    dummies_df[["id", "author"]].rename(columns={"id": "parent_id", "author": "parent_user"}),
    on="parent_id",
    how="left",
)

# Keyword word hit count
# TODO: more keywords?
patterns = {"moon": r"moon", "pump": r"pump"}
keyword_df = parent_user_df.copy()
for col, pattern in patterns.items():
    keyword_df[col] = keyword_df["body"].str.findall(
        pattern, flags=re.IGNORECASE
    ).str.len()

# Numeric columns will be aggregated later on for comments and posts
agg_columns = keyword_df.select_dtypes('number').columns.to_list()
agg_columns.remove("depth")  # not of interest to aggregated edges

# Filter for comments
comments_df = keyword_df.query("depth >= 0").copy()

# Group and aggregate comments
grouped_comments_df = comments_df.groupby(["author", "parent_user"], as_index=False).agg(
    weight=("score", "size"),
    mean_score=("score", "mean"),
    **{col: (col, "sum") for col in agg_columns},
)

# Display top edges
cols = {"weight", "score"} | set(patterns.keys())
for column in cols:
    print(f"Top edges (sorted by {column}):")
    display(grouped_comments_df.sort_values(column, ascending=False).head(5))
```

Edges between users with unknown user names are high up in the `count` and `weight` scores as they are all grouped together. Their comments are kept for now, as they might give insight, but it is important to note, that probably multiple users where put together into `None`. Edges between users with unknown names will be kept as a self loop edge on the node `None`.

`None` users and users with `mod`, `auto` or `bot` in there names are high in the list of edges sorted by `moon` or `pump` count. This might suggest, that they might not be official mods, because official mods would probably not use those words (this would have to be verified).

The edge data is now aggregated to precalculate node attributes to speed up network analysis. For each node in and out sums of edge attributes are calculated by grouping by `author` or `parent_user` accordingly. The attributes are prefixed with `out_comment` and `in_comment` accordingly. The number of posts (`post_count`), sum of scores of posts (`post_scores`) and selected word counts in posts are aggregated per author as well. A glimpse at the node data is provided:

```{python}
def prefix_column(df: pd.DataFrame, prefix: str, exclude_column: str = "author") -> pd.DataFrame:
    """
    Prefixes all column names in the DataFrame except one with a given string.

    Args:
        df: The DataFrame whose columns will be renamed.
        prefix: The string to add as a prefix to the column names.
        exclude_column: The name of the column to exclude from renaming.

    Returns:
        A new DataFrame with renamed columns.
    """
    return df.rename(columns={col: f"{prefix}{col}" for col in df.columns if col != exclude_column})


# Outgoing edge stats per node
grouped_out_comments_df = prefix_column(
    (
        grouped_comments_df.drop(columns="parent_user")
        .groupby("author", as_index=False)
        .sum()
    ),
    "out_comment_"
)


# Incomming edge stats per node
grouped_in_comments_df = prefix_column(
    (
        grouped_comments_df.drop(columns="author")
        .groupby("parent_user", as_index=False)
        .sum()
        .rename(columns={"parent_user": "author"})
    ),
    "in_comment_"
)

# Post stats per node
grouped_posts_df = prefix_column(
    (
        keyword_df.query("depth == -1")
        .copy()
        .groupby("author", as_index=False)
        .agg(
            count=("score", "size"),
            mean_score=("score", "mean"),
            **{col: (col, "sum") for col in agg_columns},
        )
    ),
    "post_"
)

# Join the node data frames
node_df = grouped_posts_df.merge(grouped_out_comments_df, 'outer').merge(grouped_in_comments_df, 'outer').fillna(0)

# Display top nodes
columns = ["post_count", "post_score"] + [
    f"{p}_comment_{x}"
    for x in patterns.keys()
    for p in ("out", "in")
]
for column in columns:
    print(f"Top nodes (sorted by {column}):")
    display(node_df.sort_values(column, ascending=False).head(5))
```


The directed graph is created:

```{python}
def create_graph_from_dataframe(df, name_column="author"):
    """
    Create a NetworkX graph with nodes from a pandas DataFrame, where one column
    is used as the node name and all other columns become node attributes.

    Args:
        df: The pandas DataFrame containing the data.
        name_column: The column to use as the node name.

    Returns:
        A NetworkX graph with nodes and their attributes.
    """
    # Initialize a graph
    G = nx.Graph()

    # Set the graph name to the column containing the network name
    G.graph["name"] = "Graph based on column: " + name_column

    # Add nodes with attributes
    G.add_nodes_from(
        (row[name_column], row.drop(name_column).to_dict()) for _, row in df.iterrows()
    )

    return G

G = create_graph_from_dataframe(node_df)


# Add edges
for _, row in grouped_comments_df.iterrows():
    edge_attributes = row[2:].to_dict()
    G.add_edge(row["author"], row["parent_user"], **edge_attributes)

# Write graph to file
output_path = "../data/processed/comments_network.gexf"
nx.write_gexf(G, output_path)
print(f"Graph saved as: {output_path}")
```

The top nodes for each edge attribute are visualized to get some insights:

```{python}
def visualize_top_nodes_graphviz(
    graph: nx.DiGraph, attribute: str, top_n: int = 10, output_file: str = "graph"
) -> None:
    """
    Exports the top N nodes based on a specified node attribute and their directed edges
    for visualization with Graphviz. Ensures arrows are displayed to indicate edge direction.

    Args:
        graph: The NetworkX directed graph.
        attribute: The node attribute to use for ranking.
        top_n: The number of top nodes to include.
        output_file: The base name for saving DOT and PNG files.
    """
    # Ensure the attribute exists in the nodes
    if not all(attribute in graph.nodes[node] for node in graph.nodes):
        raise ValueError(f"Attribute '{attribute}' not found in all nodes.")

    # Find the top N nodes by the specified attribute
    top_nodes = [node for node, _ in heapq.nlargest(top_n, graph.nodes(data=attribute), key=lambda x: x[1])]

    # Create a subgraph with only the top nodes (includes isolated nodes)
    subgraph = graph.subgraph(top_nodes)

    # Convert to PyGraphviz AGraph and force directed rendering
    A = nx.nx_agraph.to_agraph(subgraph)
    A.graph_attr["rankdir"] = "TB"  # Layout top-to-bottom
    A.graph_attr["label"] = f"Top {top_n} Nodes by {attribute.capitalize()}"
    A.graph_attr["labelloc"] = "top"
    A.graph_attr["fontsize"] = 20

    # Ensure directed graph and arrowheads
    A.graph_attr["directed"] = "true"
    for edge in A.edges():
        edge.attr["arrowhead"] = "normal"  # Explicitly set arrowhead for edges

    # Export DOT file and render as PNG
    dot_file = f"{output_file}.dot"
    png_file = f"{output_file}.png"
    A.write(dot_file)
    A.layout(prog="dot")  # Use Graphviz's dot layout
    A.draw(png_file)

    # Display the PNG in the notebook
    display(Image(filename=png_file))

node_attributes = node_df.columns.to_list()
node_attributes.remove("author")
for node_attr in node_attributes:
    visualize_top_nodes_graphviz(G, node_attr)
```

Here we can see the top 10 nodes with the most comments. They are heavily interconnected. 

TODO: what could we do else in eda?

## Analysis

Are there Users, which answer often to the same users?

```{python}
user_parent_counts = (
    df.groupby(["parent_user", "author"]).size().reset_index(name="count")
)

top_user_parent = user_parent_counts.sort_values(by="count", ascending=False).head(10)

print("Users with the most interactions with the same parent user:")
print(top_user_parent)
```

These are the User with most comments to the same user. There are many Moderator bots in the list and also None.

```{python}
comments_df = df[df["score"] > 0]

# Sort by created (assuming created is a timestamp or sortable column) and take the newest 1000 per search_query
comments_df = (
    comments_df.sort_values(by="created", ascending=False)
    .groupby("search_query")
    .head(500)
)

print(len(comments_df))
# Group by parent_user and author, summing up the scores
user_edges = comments_df.groupby(["parent_user", "author"])["score"].sum().reset_index()


# Add 1 to the summed scores for the edge weight
user_edges["weight"] = user_edges["score"] + 1

# Create a directed graph
G_weighted = nx.DiGraph()

# Add edges with calculated weights and search_query as an edge attribute
for _, row in tqdm(comments_df.iterrows(), desc="Building Graph"):
    G_weighted.add_edge(
        row["parent_user"],
        row["author"],
        weight=row["score"] + 1,
        search_query=row["search_query"],
        moon=row["moon"],  # Add 'moon' as an edge attribute
        pump=row["pump"],  # Add 'pump' as an edge attribute
    )
```

```{python}
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_weighted, seed=42, k=60, scale=3.0)

# Draw nodes and edges
nx.draw_networkx_nodes(G_weighted, pos, node_size=1, alpha=0.7)
nx.draw_networkx_edges(
    G_weighted, pos, width=1, alpha=0.5, arrowstyle="->", arrowsize=8
)

edge_labels = nx.get_edge_attributes(G_weighted, "weight")
for (u, v), label in edge_labels.items():
    x, y = (pos[u][0] + pos[v][0]) / 2, (pos[u][1] + pos[v][1]) / 2  # Midpoint of edge
    plt.text(x, y, f"{label:.1f}", fontsize=6, color="black", alpha=0.9)

plt.title(
    "User Interaction Graph with Weighted Edges (Filtered by Score > 0 and Newest 500 per Search Query)",
    fontsize=16,
)
plt.axis("off")
plt.show()
```

```{python}
G_undirected = G_weighted.to_undirected()

# Perform Label Propagation Clustering on the undirected graph
tqdm_desc = "Label Propagation Clustering"
with tqdm(total=1, desc=tqdm_desc) as pbar:
    communities = list(label_propagation_communities(G_undirected))
    pbar.update(1)

# Assign communities to nodes
node_community_mapping = {
    node: i for i, community in enumerate(communities) for node in community
}
for node, community in node_community_mapping.items():
    G_undirected.nodes[node]["community"] = community

# Display the number of clusters
num_clusters = len(communities)
print(f"Label Propagation Clustering completed: {num_clusters} clusters found.")
```

```{python}
# print size of clusters
cluster_sizes = Counter(node_community_mapping.values())
# sort
cluster_sizes = dict(sorted(cluster_sizes.items(), key=lambda x: x[1], reverse=True))
print("Size of clusters:")
print(cluster_sizes)
```

```{python}
self_edges = [(u, v) for u, v in G_undirected.edges() if u == v]
non_self_edges = [(u, v) for u, v in G_undirected.edges() if u != v]

# Create subgraphs for self-references and non-self-references
G_self = G_undirected.edge_subgraph(self_edges).copy()
G_non_self = G_undirected.edge_subgraph(non_self_edges).copy()

# Set up subplots
fig, axes = plt.subplots(1, 2, figsize=(20, 10))
fig.suptitle(
    "Self-references vs Non-self-references in User Interaction Graph (with Clusters)",
    fontsize=18,
)

# Plot self-references with clusters
ax = axes[0]
pos_self = nx.spring_layout(G_self, seed=42, k=0.1)
community_colors_self = [G_self.nodes[node]["community"] for node in G_self.nodes()]
nx.draw_networkx_nodes(
    G_self,
    pos_self,
    node_size=50,
    alpha=0.8,
    ax=ax,
    node_color=community_colors_self,
    cmap=plt.cm.Set3,
)
nx.draw_networkx_edges(G_self, pos_self, width=1.5, alpha=0.6, ax=ax)
ax.set_title("Self-references (Edges on Themselves)", fontsize=16)
ax.axis("off")

# Plot non-self-references with clusters
ax = axes[1]
pos_non_self = nx.spring_layout(G_non_self, seed=42)
community_colors_non_self = [
    G_non_self.nodes[node]["community"] for node in G_non_self.nodes()
]
nx.draw_networkx_nodes(
    G_non_self,
    pos_non_self,
    node_size=50,
    alpha=0.8,
    ax=ax,
    node_color=community_colors_non_self,
    cmap=plt.cm.Set3,
)
nx.draw_networkx_edges(G_non_self, pos_non_self, width=1.5, alpha=0.6, ax=ax)
ax.set_title("Non-self-references (Edges with No Self-references)", fontsize=16)
ax.axis("off")

plt.show()
```

```{python}
cluster_sizes = Counter(nx.get_node_attributes(G_undirected, "community").values())

# Filter for clusters greater than size 2
valid_clusters = {cluster for cluster, size in cluster_sizes.items() if size > 10}

# Filter nodes belonging to valid clusters
nodes_in_valid_clusters = [
    node
    for node in G_undirected.nodes
    if G_undirected.nodes[node]["community"] in valid_clusters
]

# Create a subgraph with only the valid clusters
G_filtered = G_undirected.subgraph(nodes_in_valid_clusters).copy()

# Plot the filtered graph
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_filtered, k=0.1, seed=42)

# Draw nodes and edges
community_colors = [G_filtered.nodes[node]["community"] for node in G_filtered.nodes()]
nx.draw_networkx_nodes(
    G_filtered,
    pos,
    node_size=10,
    alpha=0.8,
    node_color=community_colors,
    cmap=plt.cm.Set3,
)
nx.draw_networkx_edges(G_filtered, pos, width=1.5, alpha=0.6)

plt.title("Filtered User Interaction Graph (Clusters with Size > 2)", fontsize=16)
plt.axis("off")
plt.show()
```

```{python}
node_search_queries = {node: [] for node in G_undirected.nodes}

# Collect search queries for each node based on connected edges
for u, v, data in G_undirected.edges(data=True):
    if "search_query" in data:
        node_search_queries[u].append(data["search_query"])
        node_search_queries[v].append(data["search_query"])

# Assign the most common 'search_query' to each node
for node, queries in node_search_queries.items():
    most_common_query = Counter(queries).most_common(1)
    G_undirected.nodes[node]["search_query"] = (
        most_common_query[0][0] if most_common_query else "Unknown"
    )

# Identify clusters with size > 10
cluster_sizes = Counter(nx.get_node_attributes(G_undirected, "community").values())
valid_clusters = {cluster for cluster, size in cluster_sizes.items() if size > 10}

# Filter nodes belonging to valid clusters
nodes_in_valid_clusters = [
    node
    for node in G_undirected.nodes
    if G_undirected.nodes[node]["community"] in valid_clusters
]

# Create a subgraph with only the valid clusters
G_filtered = G_undirected.subgraph(nodes_in_valid_clusters).copy()

# Assign colors based on 'search_query'
search_query_colors = nx.get_node_attributes(G_filtered, "search_query")
unique_queries = list(set(search_query_colors.values()))
color_map = {query: idx for idx, query in enumerate(unique_queries)}
color_values = [color_map[search_query_colors[node]] for node in G_filtered.nodes()]

# Plot the filtered graph
plt.figure(figsize=(12, 10))
pos = nx.spring_layout(G_filtered, k=0.1)

nx.draw_networkx_nodes(
    G_filtered, pos, node_size=10, alpha=0.8, node_color=color_values, cmap=plt.cm.tab20
)
nx.draw_networkx_edges(G_filtered, pos, width=1.5, alpha=0.6)

plt.title(
    "Filtered User Interaction Graph (Clusters with Size > 10, Colored by Search Query)",
    fontsize=16,
)
plt.axis("off")
plt.show()
```

```{python}
search_queries = set(nx.get_edge_attributes(G_filtered, "search_query").values())

# Create and plot a subgraph for each search query
for search_query in search_queries:
    # Filter edges with the current search query
    edges_for_query = [
        (u, v)
        for u, v, data in G_filtered.edges(data=True)
        if data["search_query"] == search_query
    ]

    # Create a subgraph for the current search query
    G_query = G_filtered.edge_subgraph(edges_for_query).copy()

    # Get cluster colors for the nodes
    cluster_colors = [G_query.nodes[node]["community"] for node in G_query.nodes()]
    unique_clusters = list(set(cluster_colors))
    cluster_color_map = {cluster: idx for idx, cluster in enumerate(unique_clusters)}
    node_colors = [
        cluster_color_map[G_query.nodes[node]["community"]] for node in G_query.nodes()
    ]

    # Plot the graph
    plt.figure(figsize=(12, 10))
    pos = nx.spring_layout(G_query, k=0.1)

    nx.draw_networkx_nodes(
        G_query, pos, node_size=10, alpha=0.8, node_color=node_colors, cmap=plt.cm.tab20
    )
    nx.draw_networkx_edges(G_query, pos, width=1.5, alpha=0.6)

    plt.title(f"User Interaction Graph for Search Query: {search_query}", fontsize=16)
    plt.axis("off")
    plt.show()
```

```{python}
search_queries = set(nx.get_edge_attributes(G_filtered, "search_query").values())

# Create and plot a subgraph for each search query with modularity labels
for search_query in search_queries:
    # Filter edges with the current search query
    edges_for_query = [
        (u, v)
        for u, v, data in G_filtered.edges(data=True)
        if data["search_query"] == search_query
    ]

    # Create a subgraph for the current search query
    G_query = G_filtered.edge_subgraph(edges_for_query).copy()

    # Identify communities
    communities = {}
    for node in G_query.nodes:
        community = G_query.nodes[node]["community"]
        if community not in communities:
            communities[community] = set()
        communities[community].add(node)

    # Calculate modularity
    modularity_value = modularity(G_query, communities.values())

    # Get cluster colors for the nodes
    cluster_colors = [G_query.nodes[node]["community"] for node in G_query.nodes()]
    unique_clusters = list(set(cluster_colors))
    cluster_color_map = {cluster: idx for idx, cluster in enumerate(unique_clusters)}
    node_colors = [
        cluster_color_map[G_query.nodes[node]["community"]] for node in G_query.nodes()
    ]

    # Plot the graph
    plt.figure(figsize=(12, 10))
    pos = nx.spring_layout(G_query, k=0.1)

    nx.draw_networkx_nodes(
        G_query, pos, node_size=10, alpha=0.8, node_color=node_colors, cmap=plt.cm.tab20
    )
    nx.draw_networkx_edges(G_query, pos, width=1.5, alpha=0.6)

    # Add modularity label
    plt.text(
        0.5,
        0.95,
        f"Modularity: {modularity_value:.3f}",
        fontsize=14,
        color="black",
        horizontalalignment="center",
        transform=plt.gca().transAxes,
    )

    plt.title(f"User Interaction Graph for Search Query: {search_query}", fontsize=16)
    plt.axis("off")
    plt.show()
```

```{python}
def calculate_internal_density(cluster_nodes, graph):
    subgraph = graph.subgraph(cluster_nodes)
    num_edges = subgraph.number_of_edges()
    num_nodes = subgraph.number_of_nodes()
    if num_nodes <= 1:
        return 0  # Avoid division by zero
    possible_edges = num_nodes * (num_nodes - 1) / 2
    return num_edges / possible_edges if possible_edges > 0 else 0


# Get unique search queries from the graph
search_queries = set(nx.get_edge_attributes(G_filtered, "search_query").values())

# Create and plot a subgraph for each search query with internal density labels
for search_query in search_queries:
    # Filter edges with the current search query
    edges_for_query = [
        (u, v)
        for u, v, data in G_filtered.edges(data=True)
        if data["search_query"] == search_query
    ]

    # Create a subgraph for the current search query
    G_query = G_filtered.edge_subgraph(edges_for_query).copy()

    # Identify communities
    communities = {}
    for node in G_query.nodes:
        community = G_query.nodes[node]["community"]
        if community not in communities:
            communities[community] = set()
        communities[community].add(node)

    # Calculate internal density for each community
    community_densities = {}
    for community, nodes in communities.items():
        community_densities[community] = calculate_internal_density(nodes, G_query)

    # Get cluster colors for the nodes
    cluster_colors = [G_query.nodes[node]["community"] for node in G_query.nodes()]
    unique_clusters = list(set(cluster_colors))
    cluster_color_map = {cluster: idx for idx, cluster in enumerate(unique_clusters)}
    node_colors = [
        cluster_color_map[G_query.nodes[node]["community"]] for node in G_query.nodes()
    ]

    # Plot the graph
    plt.figure(figsize=(12, 10))
    pos = nx.spring_layout(G_query, k=0.1)

    nx.draw_networkx_nodes(
        G_query, pos, node_size=10, alpha=0.8, node_color=node_colors, cmap=plt.cm.tab20
    )
    nx.draw_networkx_edges(G_query, pos, width=1.5, alpha=0.6)

    # Add internal density labels for each cluster
    for community, density in community_densities.items():
        cluster_nodes = communities[community]
        cluster_center = [pos[node] for node in cluster_nodes]
        center_x = sum([coord[0] for coord in cluster_center]) / len(cluster_center)
        center_y = sum([coord[1] for coord in cluster_center]) / len(cluster_center)
        plt.text(
            center_x,
            center_y,
            f"{density:.3f}",
            fontsize=10,
            color="black",
            horizontalalignment="center",
            bbox=dict(facecolor="white", alpha=0.5, boxstyle="round,pad=0.3"),
        )

    plt.title(
        f"User Interaction Graph for Search Query: {search_query} (Internal Density)",
        fontsize=16,
    )
    plt.axis("off")
    plt.show()
```

```{python}
search_queries = set(nx.get_edge_attributes(G_filtered, "search_query").values())
modularity_values = []

for search_query in search_queries:
    # Filter edges with the current search query
    edges_for_query = [
        (u, v)
        for u, v, data in G_filtered.edges(data=True)
        if data["search_query"] == search_query
    ]

    # Create a subgraph for the current search query
    G_query = G_filtered.edge_subgraph(edges_for_query).copy()

    # Identify communities
    communities = {}
    for node in G_query.nodes:
        community = G_query.nodes[node]["community"]
        if community not in communities:
            communities[community] = set()
        communities[community].add(node)

    # Calculate modularity
    modularity_value = modularity(G_query, communities.values())
    modularity_values.append((search_query, modularity_value))

# Sort the modularity values for better visualization
modularity_values.sort(key=lambda x: x[1], reverse=True)

# Extract data for plotting
queries = [item[0] for item in modularity_values]
modularity_scores = [item[1] for item in modularity_values]

# Create the barplot
plt.figure(figsize=(12, 8))
plt.barh(queries, modularity_scores, color="skyblue", edgecolor="black")
plt.xlabel("Modularity", fontsize=14)
plt.ylabel("Search Query", fontsize=14)
plt.title("Modularity Scores for Each Search Query", fontsize=16)
plt.gca().invert_yaxis()  # Invert y-axis for better readability
plt.grid(axis="x", linestyle="--", alpha=0.7)
plt.show()
```


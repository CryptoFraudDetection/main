---
title: Scrape Reddit Posts
jupyter: python3
---

```{python}
#| ExecuteTime: {end_time: '2025-01-16T12:58:56.264221Z', start_time: '2025-01-16T12:58:56.260569Z'}
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
import json
import numpy as np
```

```{python}
#| ExecuteTime: {end_time: '2025-01-16T12:58:57.528191Z', start_time: '2025-01-16T12:58:56.264306Z'}
df = pd.read_parquet("../data/processed/reddit_posts.parquet")
```

```{python}
#| ExecuteTime: {end_time: '2025-01-16T12:58:57.771406Z', start_time: '2025-01-16T12:58:57.528191Z'}
df['created'] = pd.to_datetime(df['created'])
df.head(5)
```

## Import used Coins from Json and define Keyword

```{python}
#| ExecuteTime: {end_time: '2025-01-16T12:58:57.791493Z', start_time: '2025-01-16T12:58:57.771406Z'}
with open('../data/raw/coins.json', 'r') as f:
    coins_data = json.load(f)

coin_to_dates = {}
for coin in coins_data:
    coin_name = coin['name']
    start_date_str = coin['start_date']
    end_date_str = coin['end_date'] or '2024-11-01'
    start_date = pd.to_datetime(start_date_str)
    end_date = pd.to_datetime(end_date_str)
    coin_to_dates[coin_name] = (start_date, end_date)

keyword_to_coin = {
    'Bitcoin': 'Bitcoin',
    'Ethereum': 'Ethereum',
    'Chainlink': 'Chainlink',
    'thorchain': 'THORChain',
    '$Atom': 'Cosmos',
    'Bitforex': 'BitForex',
    '$Avax': 'Avalanche',
    'Terra Luna': 'Terra Luna',
    '$FTT': 'FTX Token',
    'Safemoon': 'Safe Moon',
    '$STA': 'STOA Network',
    'Beercoin': 'BeerCoin',
    'Teddy Doge': 'Teddy Doge'
}

df_keywords = ['Bitcoin', 'Ethereum', 'Chainlink', 'thorchain', '$Atom', 'Bitforex', '$Avax', 'Terra Luna', '$FTT', 'Safemoon', '$STA', 'Beercoin', 'Teddy Doge']

missing_keywords = set(df_keywords) - set(keyword_to_coin.keys())

if missing_keywords:
    print(f"Missing mappings for keywords: {missing_keywords}")
```

```{python}
#| ExecuteTime: {end_time: '2025-01-16T13:11:46.379239Z', start_time: '2025-01-16T13:11:44.413879Z'}
df['created'] = pd.to_datetime(df['created'])
df['date'] = df['created'].dt.date
grouped = df.groupby(['search_query', 'date']).size().reset_index(name='count')
grouped = grouped.sort_values('date')

print(grouped)
keywords = df['search_query'].unique()
N = len(keywords)

fig, axes = plt.subplots(N, 1, figsize=(12, N * 2), sharex=True)

for i, keyword in enumerate(keywords):
    ax = axes[i] if N > 1 else axes
    
    # Filter the grouped dataframe to just the current keyword, copy it
    data = grouped[grouped['search_query'] == keyword].copy()
    data.set_index('date', drop=True, inplace=True)
    
    # Only keep numeric columns for reindexing
    data = data[['count']]
    
    # Reindex over the entire date range
    full_idx = pd.date_range(start=grouped['date'].min(), 
                             end=grouped['date'].max(), freq='D')
    data = data.reindex(full_idx, fill_value=0)

    data['count_smooth'] = data['count']
    ax.plot(data.index, data['count_smooth'], label='Count (smoothed)')
    
    ax.set_title(keyword)
    ax.set_ylabel('Count')
    
    coin_name = keyword_to_coin.get(keyword)
    if coin_name:
        start_date, end_date = coin_to_dates.get(coin_name, (None, None))
        if start_date and end_date:
            ax.axvline(x=start_date, color='red', linestyle='--')
            ax.axvline(x=end_date, color='red', linestyle='--')
    
    if i == N - 1:
        ax.set_xlabel('Date')
    else:
        ax.set_xlabel('')
    ax.tick_params(axis='x', rotation=45)
    ax.grid(True)

plt.tight_layout()
plt.suptitle('Count of Search Keywords over Time', y=1.02)
plt.subplots_adjust(hspace=0.3)
plt.show()
```

In this plot we can see the Count of Tweets of every coin over the time. In red we can see the start and enddate which we are interested in.

```{python}
#| ExecuteTime: {end_time: '2025-01-16T13:14:56.066582Z', start_time: '2025-01-16T13:14:55.903988Z'}
#cut out coins of test data
test_coins = ['Ethereum', '$Atom', '$FTT', 'Safemoon']

df_train = df[~df['search_query'].isin(test_coins)]
df_train
```

```{python}
#| ExecuteTime: {end_time: '2025-01-16T13:14:57.953786Z', start_time: '2025-01-16T13:14:57.950249Z'}
words_list = ['moon', 'mooon', 'to 0', 'to zero', 'scam', 'fraud', 'rug', 'rugpull', 'pump', 'dump', 'rich', 'poor', 'millionaire']
words_list_2 = [ 'billionaire', 'invest', 'massive','time','trust', 'haters'
]
```

```{python}
#| ExecuteTime: {end_time: '2025-01-16T13:15:32.205231Z', start_time: '2025-01-16T13:14:58.908646Z'}
# Function to generate heatmaps
def create_heatmaps(df, words_list):
    for keyword in df['search_query'].unique():
        subset = df[df['search_query'] == keyword]
        
        start_date = subset['created'].min().date() - pd.Timedelta(days=10)
        end_date = subset['created'].max().date() + pd.Timedelta(days=10)
        all_dates = pd.date_range(start_date, end_date).date
        
        heatmap_data = pd.DataFrame(0, index=words_list, columns=all_dates)
        
        for _, row in subset.iterrows():
            post_date = row['created'].date()
            for word in words_list:
                if word in row['body'].lower():
                    heatmap_data.at[word, post_date] += 1

        for date in all_dates:
            if date not in heatmap_data.columns:
                heatmap_data[date] = 0
        
        plt.figure(figsize=(12, 6), facecolor='white')
        plt.title(f"Heatmap for search_query: {keyword}", fontsize=14)
        plt.xlabel("Date", fontsize=12)
        plt.ylabel("Words", fontsize=12)

        xticks_indices = np.linspace(0, len(all_dates) - 1, min(10, len(all_dates))).astype(int)
        xticks_labels = [all_dates[i] for i in xticks_indices]
        
        plt.xticks(xticks_indices, xticks_labels, rotation=45, fontsize=10)
        plt.yticks(range(len(words_list)), words_list, fontsize=10)
        
        plt.imshow(heatmap_data, aspect='auto', cmap='viridis', interpolation='nearest', origin='lower')
        plt.colorbar(label='Frequency', orientation='vertical')
        plt.tight_layout()
        plt.show()

create_heatmaps(df_train, words_list)
create_heatmaps(df_train, words_list_2)
```

In this plot we can see the Heatmap of the words in the posts over the time. The words were specific chosen from domain knowledge, that could indicate word used from scammers to promote their coins.


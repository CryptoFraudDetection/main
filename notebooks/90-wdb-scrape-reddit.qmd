---
title: WDB - Scrape Reddit Posts
author: Florian Baumgartner, Aaron BrÃ¼lisauer
execute-dir: file
output-dir: output
toc: true
number-sections: true
number-depth: 2
papersize: a4paper
code-line-numbers: true
code-fold: true
code-overflow: wrap
self-contained: true
---

# Setup

```{python}
from contextlib import contextmanager
import json
import os
from datetime import datetime
import time

from tqdm import tqdm
import pandas as pd
import matplotlib.pyplot as plt


from CryptoFraudDetection.utils import sentiment
from CryptoFraudDetection.utils.enums import ScraperNotebookMode, LoggerMode
from CryptoFraudDetection.utils.logger import Logger
from CryptoFraudDetection.scraper.reddit import RedditScraper
from CryptoFraudDetection.elasticsearch.data_insertion import insert_dataframe
from CryptoFraudDetection.elasticsearch.data_retrieval import search_data


logger = Logger(name="scrape_reddit", level=LoggerMode.INFO, log_dir="../logs")
```

# Scrape Reddit Posts

Enable READ or WRITE mode of the notebook, subreddit and coin limits and the number of DB retries:

```{python}
# Enable or disable scraping
MODE = ScraperNotebookMode.READ
# Number of subreddits and queries to scrape
# set to None to scrape all
SUBREDDIT_LIMIT = None
QUERY_LIMIT = None
DB_RETRY = 10
```

Define lists of subreddits and coins:

```{python}
# Subreddits to scrape
subreddits: list[str] = [
    "r/CryptoCurrency",
    "r/CryptoMoonShots",
    "r/CryptoMarkets",
    "r/Crypto",
    "r/Ethereum",
    "r/Bitcoin",
    "r/btc",
    "r/litecoin",
    "r/ethtrader",
    "r/Ripple",
    "r/BitcoinMarkets",
    "r/altcoin",
    "r/binance",
]

# Crypto coins to scrape
coins: list[str] = [
    "Bitcoin",
    "BTC",
    "Ethereum",
    "ETH",
    "Cosmos",
    "ATOM",
    "Avalanche",
    "AVAX",
    "FTX Token",
    "FTT",
    "Terra Luna Classic",
    "Terra Luna",
    "Terra Classic",
    "LUNC",
    "Squid-Game-Coin",
    "SQUID",
    "BeerCoin",
    "BEER",
    "BitForex",
    "BF",
    "BeerCoin",
    "BEER",
    "Safe Moon",
    "SAFEMOON",
    "Teddy Doge",
    "TEDDY V2",
    "STOA Network",
    "STA",
    "Chainlink",
    "LINK",
    "Polkadot",
    "DOT",
    "Solana",
    "SOL",
    "THORChain",
    "RUNE",
    "Avalanche",
    "AVAX",
    "Algorand",
    "ALGO",
    "Polygon",
    "MATIC",
    "Cardano",
    "ADA",
    "VeChain",
    "VET",
]
```

Shrink the lists of subreddits and coins if enabled.

```{python}
subreddits_ = subreddits[:SUBREDDIT_LIMIT] if SUBREDDIT_LIMIT else subreddits
coins_ = coins[:QUERY_LIMIT] if QUERY_LIMIT else coins
```

Define scrape functions:

```{python}
class LockFileExistsError(Exception):
    """Custom exception raised when the lock file already exists."""

    pass


@contextmanager
def acquire_lock(lock_file_path):
    """Creates and manages a lock file to ensure exclusive access.

    This context manager creates a lock file at the specified path to enforce
    a locking mechanism. If the lock file already exists, it raises a
    `LockFileExistsError`. The lock file is removed upon exiting the context.

    Args:
        lock_file_path (str): Path to the lock file to be created.

    Raises:
        LockFileExistsError: If the lock file already exists.

    Yields:
        None: Control is returned to the calling context within the lock.
    """
    if os.path.exists(lock_file_path):
        raise LockFileExistsError(f"Lock file already exists: {lock_file_path}")
    try:
        with open(lock_file_path, "w") as lock_file:
            pass
        yield
    finally:
        if os.path.exists(lock_file_path):
            os.remove(lock_file_path)


def scrape_subreddit_coin(subreddit: str, coin: str):
    """Scrapes subreddit data for a specific coin and stores it in a database.

    This function scrapes posts from a given subreddit for a specified coin, saves the data
    as a JSON file, converts it to a Pandas DataFrame, and writes the DataFrame to a database.
    It uses a locking mechanism to prevent concurrent scraping of the same subreddit-coin pair.

    Args:
        subreddit (str): The subreddit to scrape (e.g., 'r/cryptocurrency').
        coin (str): The coin keyword to search for within the subreddit.

    Raises:
        LockFileExistsError: If the lock file already exists, indicating another process
            is currently scraping the same subreddit and coin.

    Side Effects:
        - Writes scraped data to a JSON file in the `../data/reddit/` directory.
        - Writes the processed DataFrame to a database table.

    Example:
        scrape_subreddit_coin("r/cryptocurrency", "bitcoin")
    """
    # JSON file path
    sub_: str = subreddit.split("/")[-1].lower()
    coin_: str = coin.lower()
    json_path: str = f"../data/reddit/{sub_}_{coin_}.json"

    try:  # try-except block to handle lock file
        # Lock file for subreddit and coin combination
        # This is to prevent multiple processes from scraping the same subreddit and coin
        lock_file_path = json_path + ".lock"
        with acquire_lock(lock_file_path):
            # Skip scraping if file already exists
            data: list[dict] | None = None
            try:
                data = json.load(open(json_path))
                logger.info(
                    f"Skipping scraping for sub: {subreddit}, coin: {coin} as it could be read from file."
                )
            except FileNotFoundError:
                pass

            # Scrape if data is not read from file
            if data is None:
                # Scrape
                logger.info(f"Scraping for sub: {subreddit}, coin: {coin}")
                scraper = RedditScraper(logger, headless=True)
                scraper.scrape(subreddit, coin)
                data = scraper.post_data

                # Write to file
                with open(json_path, "w") as f:
                    f.write(json.dumps(data, indent=4))

            # convert to dataframe
            post_df = pd.DataFrame(data)
            post_df.rename(
                columns={"num_comment": "num_comments", "text": "body"}, inplace=True
            )
            post_df["subreddit"] = sub_
            post_df["search_query"] = coin_

            # Write to db
            for i in range(DB_RETRY):
                try:
                    if i > 0:
                        time.sleep(i**2)  # exponential backoff
                        logger.warning(
                            f"Retry {i} for writing subreddit '{subreddit}' coin '{coin}' to db."
                        )
                    insert_dataframe(logger, "wdb", post_df)
                    break
                except Exception as e:
                    logger.warning(f"Error occurred while writing to db. Error: {e}")

    # Skip if lock file is already acquired by another process
    except LockFileExistsError:
        logger.info(
            f"Skipping scraping for sub: {subreddit}, coin: {coin} as it is being scraped by another process."
        )
```

Scrape the data and write into Elasticsearch:

```{python}
if MODE == ScraperNotebookMode.WRITE:
    for subreddit in subreddits_:
        for coin in coins_:
            scrape_subreddit_coin(subreddit, coin)
```

# EDA 

Data manipulation and visualization functions:

```{python}
def visualize_subreddit_counts(scraped_subreddits_posts):
    """
    This function visualizes the counts of sraped posts per subreddit
    Args:
        scraped_subreddit_posts : [[subquery,total_hits],...]
    """
    # Create DataFrame
    df_count_posts = pd.DataFrame(
        scraped_subreddits_posts, columns=["subreddit", "count"]
    )

    # Clean subreddit names
    df_count_posts["subreddit"] = df_count_posts["subreddit"].str.replace(
        "subreddit:", "", regex=False
    )

    # Sort by count in descending order
    df_count_posts = df_count_posts.sort_values("count", ascending=False)

    # Visualize
    plt.figure(figsize=(12, 6))
    plt.bar(df_count_posts["subreddit"], df_count_posts["count"], color="skyblue")

    plt.xlabel("Subreddit")
    plt.ylabel("Number of records")
    plt.title("Number of records per subreddit")
    plt.xticks(rotation=45, ha="right")
    plt.tight_layout()
    plt.show()


def get_data(index, q):
    """
    Retrieve data from Elasticsearch

    Args:
        index:  index of elastic db
        q:      query for search in elasticsearch
    """
    # Retrieve data from the index
    response = search_data(index=index, q=q)
    hits = response["hits"]["hits"]
    return hits


def get_timeline_data(index, q):
    """
    extract date and content from data on elastic search

    Args:
        index:  index of elastic db
        q:      query for search in elasticsearch
    """
    hits = get_data(index, q)
    # Extract relevant fields
    data = []
    for hit in hits:
        source = hit["_source"]
        data.append(
            {
                "subreddit": source.get("subreddit"),
                "date": source.get("date").split("T")[0],  # Extract the date part
            }
        )

    return data


def process_data(data):
    """
    process the data from elasticsearch: Group by subreddit and date, then count posts

    Args:
        data: data from elasticsearch
    """
    df = pd.DataFrame(data)
    df["date"] = pd.to_datetime(
        df["date"]
    )  # Ensure the date column is in datetime format

    # Group by subreddit and date, then count posts
    timeline = df.groupby(["subreddit", "date"]).size().reset_index(name="post_count")
    return timeline


def plot_timeline(timeline):
    """
    plots the amount of post on a timeline per subreddit

    Args:
        timeline:   data with date and count
    """
    subreddits = timeline["subreddit"].unique()

    # Define size of figure
    num_subreddits = len(subreddits)
    fig, axes = plt.subplots(
        nrows=num_subreddits, ncols=1, figsize=(10, 4 * num_subreddits)
    )

    # If only one subplot, axes is not a list, so wrap it in a list
    if num_subreddits == 1:
        axes = [axes]

    for ax, subreddit in zip(axes, subreddits):
        subreddit_data = timeline[timeline["subreddit"] == subreddit]
        ax.plot(subreddit_data["date"], subreddit_data["post_count"])
        ax.set_title(f"Posts Per Day for {subreddit}", fontsize=14)
        ax.set_xlabel("Date", fontsize=10)
        ax.set_ylabel("Number of Posts", fontsize=10)
        ax.grid(True)

    plt.tight_layout()
    plt.show()
```

Analysis of number of posts per subreddit and on the timeline:

```{python}
# edit subredditlist
subreddits = [subreddit.replace("r/", "").lower() for subreddit in subreddits]
# build query for elasticsearch
elasticquery_subreddits = [f"subreddit:{subreddit}" for subreddit in subreddits]
# Get count of data per subreddit
scraped_subreddits_posts = []

for subquery in elasticquery_subreddits:
    # Search the database for the current subreddit query
    response = search_data(index="wdb", q=subquery)
    try:
        # Extract the total number of records for the subreddit
        total_hits = response["hits"]["total"]["value"]
        print(f"Number of records for {subquery}: {total_hits}")
    except Exception as e:
        # Handle cases where no records are found or an error occurs
        print(f"No records for query: {subquery}")
        continue

    # Append the subreddit and its record count to the results list
    scraped_subreddits_posts.append([subquery, total_hits])


# visualize count per subreddit
data_counts = visualize_subreddit_counts(scraped_subreddits_posts)
# visualize timeline count for the first 3 subreddit
data_timeline = get_timeline_data("wdb", "*")
timeline = process_data(data_timeline)
plot_timeline(timeline)
```

## Sentiment Analysis

Gather data for sentiment analysis:

```{python}
def get_data_for_sentiment(index, q):
    """
    get the relevant data for the sentiment analysis from the elastic db

    Args:
        index:  index of elastic db
        q:      query for search in elasticsearch
    """
    hits_ = get_data(index, q)

    hits_
    data_sentiment = []
    for hit in hits_:
        source = hit["_source"]
        id_ = source.get("id")
        title = source.get("title")
        title = title if isinstance(title, str) else ""
        body = source.get("body")
        body = body if isinstance(body, str) else ""
        data_sentiment.append({"id": id_, "text": body + title})
    return data_sentiment


data_sentiment = get_data_for_sentiment("wdb", "*")
```

To not create to high costs and to get responses in a reasonable time, a sample of 100 reviews is taken to perform the sentiment analysis:

```{python}
subsample_data_sentiment = data_sentiment[:100]

# get post text, make sentiment-analysis of it and save it score in the same dict
for d in tqdm(subsample_data_sentiment, desc="Sentiment Analysis"):
    text = d.get("text", "")
    if isinstance(text, str):
        try:
            score = sentiment.sentiment([text])
            d["sentiment_score"] = score
        except:
            d["sentiment_score"] = None
    else:
        d["sentiment_score"] = None

# create df from list of dicts
df_sentiment = pd.DataFrame(subsample_data_sentiment)
df_sentiment.sample(5, random_state=42)
```

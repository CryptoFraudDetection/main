---
jupyter: python3
---

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import torch
from datetime import datetime
```

```{python}
df_btc = pd.read_csv('../data/raw/coin_price_data/btc.csv')
df_link = pd.read_csv('../data/raw/coin_price_data/link.csv')
df_avax = pd.read_csv('../data/raw/coin_price_data/avax.csv')
df_beer = pd.read_csv('../data/raw/coin_price_data/beer.csv')
df_bitforex = pd.read_csv('../data/raw/coin_price_data/bitforex.csv')
df_sta = pd.read_csv('../data/raw/coin_price_data/sta.csv')
df_rune = pd.read_csv('../data/raw/coin_price_data/rune.csv')
df_teddydoge = pd.read_csv('../data/raw/coin_price_data/teddydoge.csv')
df_luna = pd.read_csv('../data/raw/coin_price_data/luna.csv')
df_safemoon = pd.read_csv('../data/raw/coin_price_data/safemoon.csv')
df_atom = pd.read_csv('../data/raw/coin_price_data/atom.csv')
df_ftx = pd.read_csv('../data/raw/coin_price_data/ftx.csv')
df_eth = pd.read_csv('../data/raw/coin_price_data/eth.csv')

#time to datetime
df_btc['time'] = pd.to_datetime(df_btc['time'])
df_link['time'] = pd.to_datetime(df_link['time'])
df_avax['time'] = pd.to_datetime(df_avax['time'])
df_beer['time'] = pd.to_datetime(df_beer['time'])
df_bitforex['time'] = pd.to_datetime(df_bitforex['time'])
df_sta['time'] = pd.to_datetime(df_sta['time'])
df_rune['time'] = pd.to_datetime(df_rune['time'])
df_teddydoge['time'] = pd.to_datetime(df_teddydoge['time'])
df_luna['time'] = pd.to_datetime(df_luna['time'])
df_safemoon['time'] = pd.to_datetime(df_safemoon['time'])
df_atom['time'] = pd.to_datetime(df_atom['time'])
df_ftx['time'] = pd.to_datetime(df_ftx['time'])
df_eth['time'] = pd.to_datetime(df_eth['time'])


coin_test = ['FTX Token', 'Safe Moon', 'Ethereum', 'Cosmos']

df_x = pd.read_parquet('../data/processed/x_posts_embeddings.parquet')
df_reddit = pd.read_parquet('../data/processed/reddit_embedded.parquet')
```

```{python}
df_btc.columns
```

```{python}
df_x.columns
```

```{python}
df_reddit.columns
```

```{python}
#all unqiue search query of x and reddit
display(df_x['searchkeyword'].unique())

display(df_reddit['search_query'].unique())
```

```{python}
#to datetime
df_x['timestamp'] = pd.to_datetime(df_x['timestamp'])
df_x.head(7)
```

```{python}
df_reddit['created'] = pd.to_datetime(df_reddit['created'])
df_reddit.head(7)
```

```{python}
#sum of na in evrey column
df_x.isna().sum()
```

```{python}
#sum of na in evrey column
df_reddit.isna().sum()
```

```{python}
def price_dfs_to_tensor(price_dfs):
    """
    Erstellt einen PyTorch-Tensor aus einer Sammlung von Preis-DataFrames.
    Jeder Coin hat seine eigene interne Zählung für die Zeitschritte.
    
    Args:
        price_dfs (dict): Ein Dictionary, bei dem der Schlüssel der Coin-Name ist
                          und der Wert ein DataFrame mit Preiszeitreihen ist.
                          Der DataFrame muss die Spalten ['time', 'open', 'high', 'low', 'close', 'volume'] enthalten.
    
    Returns:
        torch_tensor (torch.Tensor): Ein Tensor der Form [num_coins, num_features, time_steps].
        coins (list): Liste der Coins in der Reihenfolge des Tensors.
        time_indices (dict): Dictionary mit Coins als Schlüssel und deren Zeitstempel als `pd.DatetimeIndex`.
    """
    required_columns = ['time', 'open', 'high', 'low', 'close', 'volume']
    for coin, df in price_dfs.items():
        if not all(col in df.columns for col in required_columns):
            raise ValueError(f"DataFrame für {coin} fehlt eine oder mehrere benötigte Spalten: {required_columns}")
    
    # Initialisiere Variablen
    coins = list(price_dfs.keys())
    num_coins = len(coins)
    num_features = len(required_columns) - 1  # Exklusive 'time'
    max_time_steps = max(len(df) for df in price_dfs.values())  # Maximale Zeitpunkte über alle Coins
    
    # Initialisiere den Tensor
    tensor = np.zeros((num_coins, num_features, max_time_steps))
    time_indices = {}
    
    # Fülle den Tensor mit den Daten für jeden Coin
    for i, coin in enumerate(coins):
        df = price_dfs[coin].sort_values(by="time")  # Sortiere nach Zeit
        time_indices[coin] = pd.to_datetime(df["time"])  # Speichere die Zeitachse
        tensor[i, :, :len(df)] = df[['open', 'high', 'low', 'close', 'volume']].values.T  # Transponiere Features
    
    # Konvertiere zu PyTorch-Tensor
    torch_tensor = torch.tensor(tensor, dtype=torch.float32)
    
    return torch_tensor, coins, time_indices
```

```{python}
#unique searchkeywords df _x
df_x['searchkeyword'].unique()
```

```{python}
price_dict_train = {'Bitcoin': df_btc, 'Chainlink': df_link, 'Avalanche': df_avax, 'BeerCoin': df_beer, 'BitForex': df_bitforex, 'STOA Network': df_sta, 'THORChain': df_rune, 'Teddy Doge': df_teddydoge, 'Terra Luna': df_luna}
price_dict_test = {'FTX Token': df_ftx, 'Safe Moon': df_safemoon, 'Ethereum': df_eth, 'Cosmos': df_atom}
```

```{python}
train_price_tensor, train_coins, train_time_indices = price_dfs_to_tensor(price_dict_train)
test_price_tensor, test_coins, test_time_indices = price_dfs_to_tensor(price_dict_test)
```

```{python}
#display all tensor dimensions
train_price_tensor.shape, test_price_tensor.shape
```

```{python}
scam_list_train = ['BeerCoin', 'BitForex', 'STOA Network', 'Teddy Doge', 'Terra Luna']
scam_list_test = ['FTX Token', 'Safe Moon', 'Ethereum', 'Cosmos']
```

```{python}
    # Konvertiere Zeitstempel in datetime und gleiche Zeitzone an
df_x["timestamp"] = pd.to_datetime(df_x["timestamp"])
df_reddit["created"] = pd.to_datetime(df_reddit["created"]).dt.tz_localize("UTC")
```

```{python}
def combine_price_and_emb_to_tensors(price_dfs, tweets_df, reddit_df, scam_coins, embedding_dim=512):
    """
    Kombiniert die Preis-, Tweet- und Reddit-Daten in einem großen Tensor und fügt die Zielvariable (y),
    die Anzahl der Embeddings pro Zeitschritt und die zugehörigen Zeitstempel hinzu.

    Args:
        price_dfs (dict): Dictionary mit Preis-Daten pro Coin.
        tweets_df (pd.DataFrame): DataFrame mit Tweets und Embeddings.
        reddit_df (pd.DataFrame): DataFrame mit Reddit-Posts und Embeddings.
        scam_coins (list): Liste der Coins, die als Scam klassifiziert sind.
        embedding_dim (int): Dimension der Embeddings.

    Returns:
        combined_tensor (torch.Tensor): Tensor mit Preis-, Tweet- und Reddit-Features.
        y_tensor (torch.Tensor): Ziel-Tensor für die Scam-Klassifikation.
        time_steps_per_coin (dict): Zeitstempel pro Coin.
        coins (list): Liste der Coins.
    """

    # Initialisierung
    coins = list(price_dfs.keys())
    num_coins = len(coins)
    price_features = ['open', 'high', 'low', 'close', 'volume']
    num_price_features = len(price_features)

    num_features_total = (num_price_features + 3 + embedding_dim  # Tweets
                          + 2 + embedding_dim)  # Reddit (score, count, embeddings)
    max_time_steps = max(len(df) for df in price_dfs.values())
    combined_tensor = torch.zeros((num_coins, num_features_total, max_time_steps), dtype=torch.float32)
    y_tensor = torch.zeros((num_coins, max_time_steps), dtype=torch.float32)
    time_steps_per_coin = {}

    for coin_idx, coin in enumerate(coins):
        price_df = price_dfs[coin].sort_values(by="time")
        time_index = pd.to_datetime(price_df["time"]).dt.tz_localize("UTC")
        time_index = pd.DatetimeIndex(time_index)

        # Aggregation für Tweets
        coin_tweets = tweets_df[tweets_df["searchkeyword"] == coin]
        tweet_agg_array = np.zeros((len(time_index), 3 + embedding_dim))  # Likes, Comments, Count + Embeddings
        for _, row in coin_tweets.iterrows():
            interval_index = time_index.get_indexer([row["timestamp"]], method="pad")[0]
            tweet_agg_array[interval_index, 0] += row["likes"]
            tweet_agg_array[interval_index, 1] += row["comments"]
            tweet_agg_array[interval_index, 2] += 1
            tweet_agg_array[interval_index, 3:] += row["embedding"]
        for i in range(len(tweet_agg_array)):
            if tweet_agg_array[i, 2] > 0:
                tweet_agg_array[i, 3:] /= tweet_agg_array[i, 2]

        # Aggregation für Reddit-Daten (ohne num_comments)
        coin_reddit = reddit_df[reddit_df["search_query"] == coin]
        reddit_agg_array = np.zeros((len(time_index), 2 + embedding_dim))  # Score, Count + Embeddings
        for _, row in coin_reddit.iterrows():
            interval_index = time_index.get_indexer([row["created"]], method="pad")[0]
            reddit_agg_array[interval_index, 0] += row["score"]
            reddit_agg_array[interval_index, 1] += 1
            reddit_agg_array[interval_index, 2:] += row["embedded_text"]
        for i in range(len(reddit_agg_array)):
            if reddit_agg_array[i, 1] > 0:
                reddit_agg_array[i, 2:] /= reddit_agg_array[i, 1]

        # Kombiniere die Features
        combined_data = pd.concat([
            price_df[price_features].reset_index(drop=True),
            pd.DataFrame(tweet_agg_array, columns=["likes", "comments", "count"] + [f"embedding_{i}" for i in range(embedding_dim)]),
            pd.DataFrame(reddit_agg_array, columns=["score", "count"] + [f"reddit_embedding_{i}" for i in range(embedding_dim)])
        ], axis=1)

        # Speichere die kombinierten Daten im Tensor
        combined_tensor[coin_idx, :, :len(price_df)] = torch.tensor(combined_data.values.T, dtype=torch.float32)

        # Zielwerte setzen
        y_tensor[coin_idx, :len(price_df)] = 1 if coin in scam_coins else 0
        time_steps_per_coin[coin] = list(time_index)

    return combined_tensor, y_tensor, time_steps_per_coin, coins

train_tensor, y_train, train_time_steps, train_coins = combine_price_and_emb_to_tensors(price_dict_train, df_x, df_reddit, scam_list_train)
test_tensor, y_test, test_time_steps, test_coins = combine_price_and_emb_to_tensors(price_dict_test, df_x, df_reddit, scam_list_test)
```

```{python}
#display shapes
display(train_tensor.shape, y_train.shape, train_coins,
        test_tensor.shape, y_test.shape, test_coins)
```

```{python}
fig, ax1 = plt.subplots(figsize=(12, 6))
coin_num = 5
time_steps = train_tensor.shape[2]
embedding_dim = 255  # feature dim des tensors
embedding_values = train_tensor[coin_num, 7 + embedding_dim - 1, :].numpy()
ax1.plot(range(time_steps), embedding_values, color='blue', label=f"Feature Dimension {embedding_dim}")
ax1.set_xlabel("Zeitpunkte in Anzhal 4h Intervalle")
ax1.set_ylabel(f"Embedding Dimension {embedding_dim} Wert", color='blue')
ax1.tick_params(axis='y', labelcolor='blue')
ax1.legend(loc="upper left")

ax2 = ax1.twinx()
close_values = train_tensor[coin_num, 3, :].numpy()
ax2.plot(range(time_steps), close_values, color='orange', label="Preis (Close)")
ax2.set_ylabel("Preis (Close)", color='orange')
ax2.tick_params(axis='y', labelcolor='orange')
ax2.legend(loc="upper right")

plt.title(f"Embedding Dimension {embedding_dim} und Preis (Close) über Zeit (erster Coin)")
plt.grid()
plt.show()
```

